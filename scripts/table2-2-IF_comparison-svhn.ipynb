{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6a8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cbc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import ResNet18\n",
    "from src import freeze_influence, hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "target_removal_label = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4643ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def test(net, dataloader, criterion, label, include):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        net_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_data = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            if include:\n",
    "                idx = targets == label\n",
    "            else:\n",
    "                idx = targets != label\n",
    "            inputs = inputs[idx]\n",
    "            targets = targets[idx]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss * len(inputs)\n",
    "            num_data +=  len(inputs)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total * 100\n",
    "        net_loss /= num_data\n",
    "        return net_loss, accuracy\n",
    "\n",
    "\n",
    "def get_full_param_index_list(net):\n",
    "    \"\"\"\n",
    "    Return a list of parameter indices in flatten network.\n",
    "    Warning: this function only provides indices of params when the param i) has requires_grad=True and 2) belongs to nn.Linear or nn.Conv2d\n",
    "    \"\"\"\n",
    "\n",
    "    index_list = np.array([], dtype=int)\n",
    "    start_index = 0\n",
    "    for module in net.modules():\n",
    "        if not list(module.children()) == []:\n",
    "            continue\n",
    "\n",
    "        num_param = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            module_index_list = np.arange(num_param, dtype=int) + start_index\n",
    "            index_list = np.append(index_list, module_index_list)\n",
    "\n",
    "        start_index += num_param\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "def projected_influence(net, total_loss, target_loss, index_list, tol, step, max_iter):\n",
    "    full_param_index_list = get_full_param_index_list(net)\n",
    "    influence = hessians.partial_influence(\n",
    "        net, total_loss, target_loss, full_param_index_list, tol=tol, step=step, max_iter=max_iter, verbose=False\n",
    "    )\n",
    "    idx = np.isin(full_param_index_list, index_list)\n",
    "    return influence[idx], full_param_index_list[idx]\n",
    "\n",
    "def f1_score(self_acc, test_acc):\n",
    "    self_acc /= 100\n",
    "    test_acc /= 100\n",
    "    return 2 * (1 - self_acc) * test_acc / (1 - self_acc + test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329471e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet18 finished. \n",
      "    Number of parameters: 11172810\n",
      "==> Preparing data..\n",
      "Original loss and acc : 0.0299, 99.10%\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18(in_channels=1).to(device)\n",
    "net_name = \"ResNet18\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "net_path = f\"checkpoints/tab2/{net_name}/cross_entropy/ckpt_0.0.pth\"\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 1024\n",
    "num_workers = 16\n",
    "num_sample_batch = 1\n",
    "num_target_sample = 1024\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, validation=False)\n",
    "train_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "loss, acc = test(net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e343f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF - ratio: 10.0%, tol: 1e-09\n",
      "11.72 - test acc: 96.67, test loss: 0.1113 | self-acc: 0.00%, self loss: 6.1449 | Score: 0.9830853\n",
      "12.03 - test acc: 97.16, test loss: 0.1043 | self-acc: 0.00%, self loss: 6.8768 | Score: 0.9856018\n",
      "12.03 - test acc: 97.28, test loss: 0.0896 | self-acc: 0.00%, self loss: 6.0751 | Score: 0.9862290\n",
      "11.22 - test acc: 95.48, test loss: 0.1607 | self-acc: 0.00%, self loss: 5.6521 | Score: 0.9768550\n",
      "11.22 - test acc: 87.78, test loss: 0.3775 | self-acc: 0.00%, self loss: 5.9342 | Score: 0.9349238\n",
      "14.03 - test acc: 95.24, test loss: 0.1603 | self-acc: 0.20%, self loss: 4.3882 | Score: 0.9746644\n",
      "11.62 - test acc: 94.79, test loss: 0.1769 | self-acc: 0.00%, self loss: 5.1241 | Score: 0.9732438\n",
      "10.91 - test acc: 96.68, test loss: 0.1174 | self-acc: 0.00%, self loss: 6.2467 | Score: 0.9831426\n",
      "11.22 - test acc: 94.58, test loss: 0.1823 | self-acc: 0.00%, self loss: 5.7766 | Score: 0.9721320\n",
      "12.52 - test acc: 96.75, test loss: 0.1103 | self-acc: 0.00%, self loss: 5.3137 | Score: 0.9834864\n",
      "\n",
      "FIF - ratio: 10.0%, tol: 1e-09\n",
      "12.53 - test acc: 95.62, test loss: 0.1498 | self-acc: 0.00%, self loss: 4.5387 | Score: 0.9776090\n",
      "12.12 - test acc: 93.42, test loss: 0.2244 | self-acc: 0.00%, self loss: 5.2028 | Score: 0.9660036\n",
      "10.82 - test acc: 97.69, test loss: 0.0748 | self-acc: 0.00%, self loss: 6.0839 | Score: 0.9883330\n",
      "12.42 - test acc: 90.91, test loss: 0.2902 | self-acc: 0.51%, self loss: 3.3975 | Score: 0.9500546\n",
      "12.52 - test acc: 88.83, test loss: 0.3664 | self-acc: 0.00%, self loss: 5.1429 | Score: 0.9408656\n",
      "12.43 - test acc: 92.37, test loss: 0.2625 | self-acc: 0.00%, self loss: 5.5563 | Score: 0.9603412\n",
      "12.53 - test acc: 92.71, test loss: 0.2546 | self-acc: 0.00%, self loss: 5.4003 | Score: 0.9621958\n",
      "15.03 - test acc: 94.53, test loss: 0.1901 | self-acc: 0.20%, self loss: 4.0959 | Score: 0.9709347\n",
      "12.12 - test acc: 92.25, test loss: 0.2592 | self-acc: 0.00%, self loss: 5.2656 | Score: 0.9596816\n",
      "12.12 - test acc: 88.04, test loss: 0.4181 | self-acc: 0.00%, self loss: 5.1327 | Score: 0.9363685\n",
      "\n",
      "PIF - ratio: 10.0%, tol: 1e-09\n",
      "72.80 - test acc: 98.64, test loss: 0.0490 | self-acc: 0.00%, self loss: 5.9261 | Score: 0.9931335\n",
      "80.40 - test acc: 97.84, test loss: 0.0707 | self-acc: 0.00%, self loss: 4.7680 | Score: 0.9890701\n",
      "65.20 - test acc: 95.65, test loss: 0.1551 | self-acc: 0.00%, self loss: 6.3814 | Score: 0.9777828\n",
      "77.70 - test acc: 91.54, test loss: 0.2837 | self-acc: 0.20%, self loss: 4.3541 | Score: 0.9548957\n",
      "72.80 - test acc: 95.43, test loss: 0.1561 | self-acc: 0.00%, self loss: 6.8684 | Score: 0.9766228\n",
      "75.30 - test acc: 98.49, test loss: 0.0511 | self-acc: 0.00%, self loss: 5.7414 | Score: 0.9924022\n",
      "70.30 - test acc: 98.38, test loss: 0.0580 | self-acc: 0.00%, self loss: 4.9624 | Score: 0.9918390\n",
      "77.80 - test acc: 92.93, test loss: 0.2524 | self-acc: 0.00%, self loss: 4.5516 | Score: 0.9633291\n",
      "72.80 - test acc: 97.02, test loss: 0.1035 | self-acc: 0.00%, self loss: 6.0294 | Score: 0.9848596\n",
      "70.20 - test acc: 97.43, test loss: 0.0869 | self-acc: 0.00%, self loss: 6.1250 | Score: 0.9869692\n",
      "\n",
      "GIF - ratio: 30.0%, tol: 1e-09\n",
      "11.62 - test acc: 81.93, test loss: 0.6139 | self-acc: 0.00%, self loss: 6.0856 | Score: 0.9006461\n",
      "12.12 - test acc: 91.97, test loss: 0.2610 | self-acc: 0.00%, self loss: 5.4920 | Score: 0.9581793\n",
      "10.91 - test acc: 95.55, test loss: 0.1489 | self-acc: 0.00%, self loss: 6.5118 | Score: 0.9772611\n",
      "12.12 - test acc: 84.14, test loss: 0.5275 | self-acc: 0.00%, self loss: 5.2587 | Score: 0.9138865\n",
      "12.12 - test acc: 94.80, test loss: 0.1946 | self-acc: 0.00%, self loss: 5.5390 | Score: 0.9733022\n",
      "11.22 - test acc: 92.39, test loss: 0.2691 | self-acc: 0.00%, self loss: 6.9139 | Score: 0.9604611\n",
      "11.21 - test acc: 89.07, test loss: 0.3689 | self-acc: 0.00%, self loss: 5.7479 | Score: 0.9421701\n",
      "10.91 - test acc: 96.11, test loss: 0.1276 | self-acc: 0.00%, self loss: 5.5558 | Score: 0.9801527\n",
      "12.53 - test acc: 91.63, test loss: 0.2919 | self-acc: 0.00%, self loss: 4.8245 | Score: 0.9563104\n",
      "11.22 - test acc: 97.28, test loss: 0.0978 | self-acc: 0.00%, self loss: 5.9023 | Score: 0.9862290\n",
      "\n",
      "FIF - ratio: 30.0%, tol: 1e-09\n",
      "10.91 - test acc: 78.37, test loss: 0.7741 | self-acc: 0.00%, self loss: 6.6855 | Score: 0.8787069\n",
      "11.63 - test acc: 95.78, test loss: 0.1408 | self-acc: 0.00%, self loss: 6.1337 | Score: 0.9784197\n",
      "11.62 - test acc: 93.29, test loss: 0.2400 | self-acc: 0.00%, self loss: 6.1005 | Score: 0.9652917\n",
      "11.71 - test acc: 68.75, test loss: 1.3835 | self-acc: 0.00%, self loss: 7.0020 | Score: 0.8148245\n",
      "11.32 - test acc: 94.37, test loss: 0.2014 | self-acc: 0.00%, self loss: 6.4650 | Score: 0.9710178\n",
      "11.71 - test acc: 93.55, test loss: 0.2328 | self-acc: 0.00%, self loss: 5.4231 | Score: 0.9666552\n",
      "12.53 - test acc: 94.20, test loss: 0.2004 | self-acc: 0.00%, self loss: 5.5250 | Score: 0.9701365\n",
      "11.72 - test acc: 85.11, test loss: 0.5118 | self-acc: 0.00%, self loss: 5.7436 | Score: 0.9195471\n",
      "11.31 - test acc: 91.94, test loss: 0.2883 | self-acc: 0.00%, self loss: 5.2071 | Score: 0.9579987\n",
      "13.33 - test acc: 93.58, test loss: 0.2104 | self-acc: 0.00%, self loss: 5.5462 | Score: 0.9668328\n",
      "\n",
      "PIF - ratio: 30.0%, tol: 1e-09\n",
      "65.10 - test acc: 78.74, test loss: 0.7559 | self-acc: 0.00%, self loss: 6.3681 | Score: 0.8810720\n",
      "62.70 - test acc: 87.91, test loss: 0.4039 | self-acc: 0.00%, self loss: 6.7296 | Score: 0.9356780\n",
      "67.60 - test acc: 94.87, test loss: 0.1839 | self-acc: 0.00%, self loss: 5.6372 | Score: 0.9736528\n",
      "67.60 - test acc: 81.90, test loss: 0.6350 | self-acc: 0.00%, self loss: 5.2192 | Score: 0.9005121\n",
      "62.60 - test acc: 85.78, test loss: 0.4768 | self-acc: 0.00%, self loss: 7.2779 | Score: 0.9234810\n",
      "65.10 - test acc: 75.25, test loss: 0.9391 | self-acc: 0.00%, self loss: 7.5707 | Score: 0.8587699\n",
      "65.10 - test acc: 93.56, test loss: 0.2136 | self-acc: 0.00%, self loss: 6.9902 | Score: 0.9667144\n",
      "62.60 - test acc: 91.05, test loss: 0.3089 | self-acc: 0.00%, self loss: 6.6138 | Score: 0.9531604\n",
      "67.70 - test acc: 79.04, test loss: 0.7261 | self-acc: 0.00%, self loss: 6.2923 | Score: 0.8829431\n",
      "65.20 - test acc: 79.90, test loss: 0.7075 | self-acc: 0.00%, self loss: 6.3190 | Score: 0.8882451\n",
      "\n",
      "GIF - ratio: 50.0%, tol: 1e-09\n",
      "12.53 - test acc: 96.53, test loss: 0.1246 | self-acc: 0.00%, self loss: 5.7175 | Score: 0.9823393\n",
      "11.31 - test acc: 88.72, test loss: 0.3898 | self-acc: 0.10%, self loss: 5.9670 | Score: 0.9397929\n",
      "10.91 - test acc: 88.49, test loss: 0.4152 | self-acc: 0.00%, self loss: 7.2004 | Score: 0.9389340\n",
      "12.43 - test acc: 97.45, test loss: 0.0862 | self-acc: 0.00%, self loss: 5.7767 | Score: 0.9870830\n",
      "11.22 - test acc: 96.45, test loss: 0.1167 | self-acc: 0.00%, self loss: 5.9313 | Score: 0.9819372\n",
      "11.32 - test acc: 94.91, test loss: 0.1767 | self-acc: 0.00%, self loss: 5.5930 | Score: 0.9738863\n",
      "11.72 - test acc: 94.74, test loss: 0.1920 | self-acc: 0.00%, self loss: 5.8311 | Score: 0.9730099\n",
      "10.81 - test acc: 88.99, test loss: 0.3834 | self-acc: 0.00%, self loss: 7.5308 | Score: 0.9417356\n",
      "12.52 - test acc: 96.09, test loss: 0.1287 | self-acc: 0.00%, self loss: 5.4969 | Score: 0.9800373\n",
      "\n",
      "FIF - ratio: 50.0%, tol: 1e-09\n",
      "10.51 - test acc: 88.26, test loss: 0.4160 | self-acc: 0.00%, self loss: 7.7546 | Score: 0.9376215\n",
      "12.43 - test acc: 91.47, test loss: 0.2962 | self-acc: 0.00%, self loss: 4.9771 | Score: 0.9554642\n",
      "11.22 - test acc: 90.15, test loss: 0.3245 | self-acc: 0.00%, self loss: 6.7546 | Score: 0.9482155\n",
      "11.31 - test acc: 88.27, test loss: 0.4090 | self-acc: 0.00%, self loss: 5.6633 | Score: 0.9376841\n",
      "11.31 - test acc: 79.09, test loss: 0.7084 | self-acc: 0.00%, self loss: 5.9947 | Score: 0.8832198\n",
      "10.82 - test acc: 93.46, test loss: 0.2137 | self-acc: 0.00%, self loss: 7.1588 | Score: 0.9661814\n",
      "13.33 - test acc: 95.97, test loss: 0.1282 | self-acc: 0.00%, self loss: 5.3929 | Score: 0.9794602\n",
      "11.22 - test acc: 95.38, test loss: 0.1602 | self-acc: 0.00%, self loss: 6.2290 | Score: 0.9763324\n",
      "12.03 - test acc: 94.71, test loss: 0.1775 | self-acc: 0.00%, self loss: 6.2801 | Score: 0.9728344\n",
      "12.83 - test acc: 93.25, test loss: 0.2210 | self-acc: 0.00%, self loss: 4.9754 | Score: 0.9650542\n",
      "\n",
      "PIF - ratio: 50.0%, tol: 1e-09\n",
      "65.10 - test acc: 94.17, test loss: 0.2024 | self-acc: 0.00%, self loss: 5.7603 | Score: 0.9699600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.10 - test acc: 95.81, test loss: 0.1475 | self-acc: 0.00%, self loss: 6.4447 | Score: 0.9785933\n",
      "65.20 - test acc: 85.75, test loss: 0.4414 | self-acc: 0.00%, self loss: 6.0647 | Score: 0.9232882\n",
      "62.60 - test acc: 67.42, test loss: 1.5360 | self-acc: 0.00%, self loss: 8.3634 | Score: 0.8054047\n",
      "62.60 - test acc: 95.62, test loss: 0.1412 | self-acc: 0.00%, self loss: 6.7454 | Score: 0.9776090\n",
      "62.70 - test acc: 67.99, test loss: 1.0798 | self-acc: 0.00%, self loss: 6.4715 | Score: 0.8094264\n",
      "62.70 - test acc: 82.17, test loss: 0.6002 | self-acc: 0.00%, self loss: 8.8793 | Score: 0.9021183\n",
      "65.20 - test acc: 87.54, test loss: 0.3804 | self-acc: 0.00%, self loss: 5.8264 | Score: 0.9335383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removal_inputs = list()\n",
    "removal_targets = list()\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    idx = targets == target_removal_label\n",
    "    removal_inputs.append(inputs[idx])\n",
    "    removal_targets.append(targets[idx])\n",
    "removal_inputs = torch.cat(removal_inputs)\n",
    "removal_targets = torch.cat(removal_targets)\n",
    "\n",
    "ratio_list = [.1, .3, .5]\n",
    "result_list_GIF = []\n",
    "result_list_FIF = []\n",
    "result_list_PIF = []\n",
    "\n",
    "tol = 1e-9\n",
    "\n",
    "for param_ratio in ratio_list:\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            if_name = \"GIF\"\n",
    "        elif i == 1:\n",
    "            if_name = \"FIF\"\n",
    "        else:\n",
    "            if_name = \"PIF\"\n",
    "\n",
    "        print(f\"{if_name} - ratio: {param_ratio*100}%, tol: {tol}\")\n",
    "        for _ in range(10):\n",
    "            # Initialize network\n",
    "            net = load_net(net, net_path)\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                if batch_idx >= num_sample_batch:\n",
    "                    break\n",
    "                idx = targets != target_removal_label\n",
    "                inputs, targets = inputs[idx], targets[idx]\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                total_loss += criterion(outputs, targets)\n",
    "\n",
    "            # Sampling the target removal data\n",
    "            sample_idx = np.random.choice(len(removal_inputs), num_target_sample, replace=False)\n",
    "            sample_removal_inputs = removal_inputs[sample_idx]\n",
    "            sample_removal_targets = removal_targets[sample_idx]\n",
    "            \n",
    "            # Make hooks\n",
    "            net_parser = selection.TopNActivations(net, param_ratio)\n",
    "            net_parser.register_hooks()\n",
    "\n",
    "            # Compute target loss\n",
    "            target_loss = (\n",
    "                criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                * len(removal_inputs)\n",
    "                / (len(train_loader.dataset) - len(removal_inputs))\n",
    "            )\n",
    "            \n",
    "            # Delete hooks\n",
    "            index_list = net_parser.get_parameters()\n",
    "            net_parser.remove_hooks()\n",
    "\n",
    "            if i == 0:\n",
    "                influence = hessians.partial_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol=tol, step=1, max_iter=30\n",
    "                )\n",
    "            elif i == 1:\n",
    "                influence = freeze_influence.freeze_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol=tol, step=1, max_iter=30\n",
    "                )\n",
    "            else:\n",
    "                influence, index_list = projected_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol=tol, step=5, max_iter=30\n",
    "                )\n",
    "\n",
    "            scale = 10 if i != 2 else 60\n",
    "            last_score = -1\n",
    "            while True:\n",
    "                utils.update_network(net, influence * scale, index_list)\n",
    "\n",
    "                self_loss, self_acc = test(net, test_loader, criterion, target_removal_label, True)\n",
    "                exclusive_loss, exclusive_acc = test(net, test_loader, criterion, target_removal_label, False)\n",
    "                score = f1_score(self_acc, exclusive_acc)\n",
    "                \n",
    "\n",
    "                if last_score > score and score > .8:\n",
    "                    print(\n",
    "                    f\"{scale:.2f} - test acc: {exclusive_acc:2.2f}, test loss: {exclusive_loss:.4f} | self-acc: {self_acc:2.2f}%, self loss: {self_loss:.4f} | Score: {score:.7f}\"\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    if score < .7:\n",
    "                        scale += .4 if i != 2 else 2.5\n",
    "                    elif score > .85:\n",
    "                        scale += .01 if i != 2 else 0.1\n",
    "                    else:\n",
    "                        scale += .1\n",
    "                    if i == 0:\n",
    "                        result_list_GIF += [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "                    elif i == 1:\n",
    "                        result_list_FIF += [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "                    else:\n",
    "                        result_list_PIF += [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "\n",
    "                last_score = score\n",
    "                \n",
    "                if scale > 90:\n",
    "                    break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3db0d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nretrained_net = DenseNet121().to(device)\\nnet_name = retrained_net.__class__.__name__\\nnet_path = f\"../checkpoints/Figure_4/{net_name}/cross_entropy/ckpt_0.0_retrained.pth\"\\nretrained_net = load_net(retrained_net, net_path)\\nflatten = False\\n\\nloss, acc = test(retrained_net, test_loader, criterion, 11, False)\\nprint(\\n    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\\n)\\nself_loss, self_acc = test(retrained_net, test_loader, criterion, 8, True)\\nexclusive_loss, exclusive_acc = test(retrained_net, test_loader, criterion, 8, False)\\nprint(\\n    f\"Retrained model \\t Self: {self_loss:.2f} {self_acc:2.2f}% | Exclusive loss: {exclusive_loss:.2f}, {exclusive_acc:2.2f}%\"\\n)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "retrained_net = DenseNet121().to(device)\n",
    "net_name = retrained_net.__class__.__name__\n",
    "net_path = f\"../checkpoints/Figure_4/{net_name}/cross_entropy/ckpt_0.0_retrained.pth\"\n",
    "retrained_net = load_net(retrained_net, net_path)\n",
    "flatten = False\n",
    "\n",
    "loss, acc = test(retrained_net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")\n",
    "self_loss, self_acc = test(retrained_net, test_loader, criterion, 8, True)\n",
    "exclusive_loss, exclusive_acc = test(retrained_net, test_loader, criterion, 8, False)\n",
    "print(\n",
    "    f\"Retrained model \\t Self: {self_loss:.2f} {self_acc:2.2f}% | Exclusive loss: {exclusive_loss:.2f}, {exclusive_acc:2.2f}%\"\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
