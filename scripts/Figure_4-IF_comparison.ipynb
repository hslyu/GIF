{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6cbc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import cifar10\n",
    "from models import VGG11\n",
    "from src import freeze_influence, hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4643ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def test(net, dataloader, criterion, label, include):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        net_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_data = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            if include:\n",
    "                idx = targets == label\n",
    "            else:\n",
    "                idx = targets != label\n",
    "            inputs = inputs[idx]\n",
    "            targets = targets[idx]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss * len(inputs)\n",
    "            num_data +=  len(inputs)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total * 100\n",
    "        net_loss /= num_data\n",
    "        return net_loss, accuracy\n",
    "\n",
    "\n",
    "def get_full_param_list(net):\n",
    "    \"\"\"\n",
    "    Return a list of parameter indices in flatten network.\n",
    "    Warning: this function only provides indices of params when the param i) has requires_grad=True and 2) belongs to nn.Linear or nn.Conv2d\n",
    "    \"\"\"\n",
    "\n",
    "    index_list = np.array([], dtype=int)\n",
    "    start_index = 0\n",
    "    for module in net.modules():\n",
    "        if not list(module.children()) == []:\n",
    "            continue\n",
    "\n",
    "        num_param = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            module_index_list = np.arange(num_param, dtype=int) + start_index\n",
    "            index_list = np.append(index_list, module_index_list)\n",
    "\n",
    "        start_index += num_param\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "def projected_influence(net, index_list, total_loss, target_loss, tol, step):\n",
    "    full_param_list = get_full_param_list(net)\n",
    "    influence = hessians.partial_influence(\n",
    "        full_param_list, target_loss, total_loss, net, tol=tol, step=step\n",
    "    )\n",
    "    idx = np.isin(full_param_list, index_list)\n",
    "    return influence[idx], full_param_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4d7719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building VGG finished. \n",
      "    Number of parameters: 9231114\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "net = VGG11().to(device)\n",
    "flatten = False\n",
    "net_name = net.__class__.__name__\n",
    "\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "net_path = f\"../checkpoints/Figure_4/{net_name}/cross_entropy/ckpt_0.0.pth\"\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 12\n",
    "num_sample_batch = 4\n",
    "num_target_sample = 500\n",
    "\n",
    "data_loader = cifar10.CIFAR10DataLoader(batch_size, num_workers, flatten=flatten)\n",
    "train_loader, val_loader, test_loader = data_loader.get_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d75f3af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss and acc : 0.3265, 91.93%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = test(net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf6cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_list = list()\n",
    "targets_list = list()\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    if batch_idx < num_sample_batch:\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "data = list()\n",
    "target = list()\n",
    "for batch_idx, (data_raw, target_raw) in enumerate(train_loader):\n",
    "    idx = target_raw == 8\n",
    "    data_raw = data_raw[idx]\n",
    "    target_raw = target_raw[idx]\n",
    "    data.append(data_raw)\n",
    "    target.append(target_raw)\n",
    "data = torch.cat(data)\n",
    "target = torch.cat(target)\n",
    "sample_idx = np.random.choice(len(data), num_target_sample, replace=False)\n",
    "sample_data = data[sample_idx]\n",
    "sample_target = target[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dbacd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [.1, .3, .5]\n",
    "scale_list = [63, 77, 150]\n",
    "tol_list = [2e-8, 1.5e-8, 1.1e-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e343f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 10.0%, tol: 2e-08, scale: 63\n",
      "Computing partial influence ... [4/10000], Tolerance: 1.831E-08, Avg. computing time: 0.622s          \n",
      "Projected\t Self: 0.5190 87.40% | Exclusive loss: 0.3454, 91.54%\n",
      "\n",
      "Ratio: 30.0%, tol: 1.5e-08, scale: 77\n",
      "Computing partial influence ... [6/10000], Tolerance: 1.477E-08, Avg. computing time: 0.624s          \n",
      "Projected\t Self: 3.4015 39.80% | Exclusive loss: 0.5643, 86.41%\n",
      "\n",
      "Ratio: 50.0%, tol: 1.1e-08, scale: 150\n",
      "Computing partial influence ... [12/10000], Tolerance: 1.077E-08, Avg. computing time: 0.625s          \n",
      "Projected\t Self: 10.6965 0.00% | Exclusive loss: 4.8023, 25.16%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param_ratio, tol, scale in zip(ratio_list, tol_list, scale_list):\n",
    "    print(f\"Ratio: {param_ratio*100}%, tol: {tol}, scale: {scale}\")\n",
    "    for i in range(3):\n",
    "        if i != 1:\n",
    "            continue\n",
    "        net = load_net(net, net_path)\n",
    "        total_loss = 0\n",
    "        for inputs, targets in zip(inputs_list, targets_list):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_sample_batch\n",
    "\n",
    "        # Make hooks\n",
    "        percentage = 0.1\n",
    "        net_parser = selection.TopNActivations(net, param_ratio)\n",
    "        # net_parser = selection.TopNGradients(net, int(num_param * percentage))\n",
    "        # net_parser = selection.RandomSelection(net, int(num_param * percentage))\n",
    "        # net_parser = selection.Threshold(net, int(num_param * percentage), 1)\n",
    "        net_parser.register_hooks()\n",
    "\n",
    "\n",
    "        target_loss = (\n",
    "            criterion(net(sample_data.to(device)), sample_target.to(device))\n",
    "            * len(data)\n",
    "            / len(train_loader.dataset)\n",
    "        )\n",
    "        target_loss.backward(retain_graph=True)\n",
    "        data_ratio = len(train_loader.dataset) / (len(train_loader.dataset) - len(data))\n",
    "        newton_loss = total_loss * data_ratio - target_loss * (1 - data_ratio)\n",
    "\n",
    "        index_list = net_parser.get_parameters()\n",
    "\n",
    "        if i == 0:\n",
    "            influence = hessians.partial_influence(\n",
    "                index_list, target_loss, total_loss, net, tol=tol, step=3\n",
    "            )\n",
    "            if_name = \"PIF\"\n",
    "        elif i == 1:\n",
    "            normalizer = 1\n",
    "#             if param_ratio == .1:\n",
    "#                 normalizer = 1.5\n",
    "#             elif param_ratio == .3:\n",
    "#                 normalizer = 3\n",
    "#             else:\n",
    "#                 normalizer = 3\n",
    "            influence, index_list = projected_influence(\n",
    "                net, index_list, total_loss, target_loss, tol=tol/normalizer, step=3\n",
    "            )\n",
    "            influence *= 10\n",
    "            if_name = \"Projected\"\n",
    "        else:\n",
    "            normalizer = 1\n",
    "            if param_ratio == .3:\n",
    "                normalizer = 1.5\n",
    "            influence = freeze_influence.freeze_influence(\n",
    "                index_list, target_loss, total_loss, net, tol=tol/(1.1*normalizer), step=3\n",
    "            )\n",
    "            if_name = \"Frozen\"\n",
    "        utils.update_network(net, influence * scale, index_list)\n",
    "        net_parser.remove_hooks()\n",
    "        save_path = f\"../checkpoints/Figure_4/{if_name}/{net_name}/cross_entropy/ckpt_0.0_{param_ratio}.pth\"\n",
    "        save_net(net, save_path)\n",
    "\n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "        print(\n",
    "            f\"{if_name}\\t Self: {self_loss:.4f} {self_acc:2.2f}% | Exclusive loss: {exclusive_loss:.4f}, {exclusive_acc:2.2f}%\"\n",
    "        )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3db0d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original loss and acc : 1.1971, 82.21%\n",
      "Retrained model \t Self: 8.72 0.00% | Exclusive loss: 0.36, 91.34%\n"
     ]
    }
   ],
   "source": [
    "retrained_net = VGG11().to(device)\n",
    "net_name = retrained_net.__class__.__name__\n",
    "net_path = f\"../checkpoints/Figure_4/{net_name}/cross_entropy/ckpt_0.0_retrained.pth\"\n",
    "retrained_net = load_net(retrained_net, net_path)\n",
    "flatten = False\n",
    "\n",
    "loss, acc = test(retrained_net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")\n",
    "self_loss, self_acc = test(retrained_net, test_loader, criterion, 8, True)\n",
    "exclusive_loss, exclusive_acc = test(retrained_net, test_loader, criterion, 8, False)\n",
    "print(\n",
    "    f\"Retrained model \\t Self: {self_loss:.2f} {self_acc:2.2f}% | Exclusive loss: {exclusive_loss:.2f}, {exclusive_acc:2.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a75f8d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PIF\t\t Self: 3.42 0.30% | Exclusive loss: 0.33, 90.49% | Influence error: 34.790283203125\n",
      "  Frozen\t Self: 2.67 38.80% | Exclusive loss: 0.35, 90.51% | Influence error: 34.789642333984375\n",
      "  Projected\t Self: 0.52 87.40% | Exclusive loss: 0.35, 91.54% | Influence error: 34.790428161621094\n",
      "\n",
      "  PIF\t\t Self: 4.49 0.30% | Exclusive loss: 0.36, 89.37% | Influence error: 34.79145431518555\n",
      "  Frozen\t Self: 3.19 31.20% | Exclusive loss: 0.37, 89.50% | Influence error: 34.79093551635742\n",
      "  Projected\t Self: 3.40 39.80% | Exclusive loss: 0.56, 86.41% | Influence error: 34.79096984863281\n",
      "\n",
      "  PIF\t\t Self: 5.48 1.60% | Exclusive loss: 0.57, 83.39% | Influence error: 34.79170608520508\n",
      "  Frozen\t Self: 4.79 12.20% | Exclusive loss: 0.52, 85.39% | Influence error: 34.79135513305664\n",
      "  Projected\t Self: 10.70 0.00% | Exclusive loss: 4.80, 25.16% | Influence error: 34.799659729003906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_inf_err(model1, model2):\n",
    "    l2_norm = 0.0\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        diff = param1 - param2\n",
    "        l2_norm += torch.norm(diff, p=2) ** 2\n",
    "    l2_norm = torch.sqrt(l2_norm)\n",
    "    return l2_norm\n",
    "\n",
    "for param_ratio, tol, scale in zip(ratio_list, tol_list, scale_list):\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            if_name = \"PIF\"\n",
    "        elif i == 1:\n",
    "            if_name = \"Frozen\"\n",
    "        else:\n",
    "            if_name = \"Projected\"\n",
    "\n",
    "        save_path = f\"../checkpoints/Figure_4/{if_name}/{net_name}/cross_entropy/ckpt_0.0_{param_ratio}.pth\"\n",
    "        net = load_net(net, save_path)\n",
    "        influence_err = calculate_inf_err(net, retrained_net)\n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "        if if_name == \"PIF\":\n",
    "            if_name += \"\\t\"\n",
    "        print(\n",
    "            f\"  {if_name}\\t Self: {self_loss:.2f} {self_acc:02.2f}% | Exclusive loss: {exclusive_loss:.2f}, {exclusive_acc:2.2f}% | Influence error: {influence_err}\"\n",
    "        )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35c477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
