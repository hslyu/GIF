{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caff5365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cbc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import ResNet18\n",
    "from src import freeze_influence, hessians, selection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "target_removal_label = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4643ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def test(net, dataloader, criterion, label, include):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        net_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_data = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            if include:\n",
    "                idx = targets == label\n",
    "            else:\n",
    "                idx = targets != label\n",
    "            inputs = inputs[idx]\n",
    "            targets = targets[idx]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss * len(inputs)\n",
    "            num_data +=  len(inputs)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total * 100\n",
    "        net_loss /= num_data\n",
    "        return net_loss, accuracy\n",
    "\n",
    "def influence_test(net, dataloader, criterion, target_label):\n",
    "    def sample_test(net, criterion, inputs, targets):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct = predicted.eq(targets).sum().item()\n",
    "\n",
    "            return loss, correct\n",
    "\n",
    "    self_loss = 0\n",
    "    self_correct = 0\n",
    "    num_self_inputs = 0\n",
    "    \n",
    "    exclusive_loss = 0\n",
    "    exclusive_correct = 0\n",
    "    num_exclusive_inputs = 0\n",
    "    \n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        target_idx = (targets == target_label)\n",
    "        batch_self_loss, batch_self_correct = sample_test(net, criterion, inputs[target_idx], targets[target_idx])\n",
    "        batch_exclusive_loss, batch_exclusive_correct = sample_test(net, criterion, inputs[~target_idx], targets[~target_idx])\n",
    "        \n",
    "        len_self_batch = len(inputs[target_idx])\n",
    "        self_loss += batch_self_loss * len_self_batch\n",
    "        self_correct += batch_self_correct\n",
    "        num_self_inputs += len_self_batch\n",
    "        \n",
    "        len_exclusive_batch = len(inputs[~target_idx])\n",
    "        exclusive_loss += batch_exclusive_loss * len_exclusive_batch\n",
    "        exclusive_correct += batch_exclusive_correct\n",
    "        num_exclusive_inputs += len_exclusive_batch\n",
    "        \n",
    "    self_loss /= num_self_inputs\n",
    "    self_acc = self_correct / num_self_inputs * 100\n",
    "    exclusive_loss /= num_exclusive_inputs\n",
    "    exclusive_acc = exclusive_correct / num_exclusive_inputs * 100\n",
    "    \n",
    "    return self_loss, self_acc, exclusive_loss, exclusive_acc\n",
    "\n",
    "def projected_influence(net, total_loss, target_loss, index_list, tol, step, max_iter, verbose):\n",
    "    num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    full_param_index_list = np.arange(num_param)\n",
    "    influence = hessians.generalized_influence(\n",
    "        net, total_loss, target_loss, full_param_index_list, tol=tol, step=step, max_iter=max_iter, verbose=verbose\n",
    "    )\n",
    "    return influence[idx]\n",
    "\n",
    "def f1_score(self_acc, test_acc):\n",
    "    self_acc /= 100\n",
    "    test_acc /= 100\n",
    "    if self_acc == 1 and test_acc == 0:\n",
    "        return 0\n",
    "    return 2 * (1 - self_acc) * test_acc / (1 - self_acc + test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d193c3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet18 finished. \n",
      "    Number of parameters: 11172810\n",
      "==> Preparing data..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m mnist\u001b[38;5;241m.\u001b[39mMNISTDataLoader(batch_size, num_workers, validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mget_data_loaders()\n\u001b[0;32m---> 29\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal loss and acc : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(net, dataloader, criterion, label, include)\u001b[0m\n\u001b[1;32m     24\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m num_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include:\n\u001b[1;32m     28\u001b[0m         idx \u001b[38;5;241m=\u001b[39m targets \u001b[38;5;241m==\u001b[39m label\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/GIF/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "net_name = \"ResNet18\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "net_path = f\"checkpoints/tab2/{net_name}/cross_entropy/ckpt_0.0.pth\"\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 16\n",
    "num_sample_batch = 1\n",
    "num_target_sample = 512\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, validation=False)\n",
    "train_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "loss, acc = test(net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e343f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "num_exp = 10\n",
    "\n",
    "removal_inputs = list()\n",
    "removal_targets = list()\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    idx = targets == target_removal_label\n",
    "    removal_inputs.append(inputs[idx])\n",
    "    removal_targets.append(targets[idx])\n",
    "removal_inputs = torch.cat(removal_inputs)\n",
    "removal_targets = torch.cat(removal_targets)\n",
    "\n",
    "ratio_list = [.1, .3, .5]\n",
    "result_list_TopNActivations = []\n",
    "result_list_TopNGradients = []\n",
    "result_list_Random = []\n",
    "result_list_Threshold = []\n",
    "tol = 1e-9\n",
    "\n",
    "for _ in range(num_exp):\n",
    "    sample_idx = np.random.choice(len(removal_inputs), num_target_sample, replace=False)\n",
    "    for param_ratio in ratio_list:\n",
    "        for i in range(4):\n",
    "            if i == 0:\n",
    "                parser = selection.TopNActivations\n",
    "            elif i == 1:\n",
    "                parser = selection.TopNGradients\n",
    "            elif i == 2:\n",
    "                parser = selection.Random\n",
    "            else:\n",
    "                parser = selection.Threshold\n",
    "\n",
    "            print(f\"{parser.__name__} - ratio: {param_ratio*100}%\")\n",
    "            # Initialize network\n",
    "            net = load_net(net, net_path)\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                if batch_idx >= num_sample_batch:\n",
    "                    break\n",
    "                idx = targets != target_removal_label\n",
    "                inputs, targets = inputs[idx], targets[idx]\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                total_loss += criterion(outputs, targets)\n",
    "\n",
    "            # Sampling the target removal data\n",
    "            sample_removal_inputs = removal_inputs[sample_idx]\n",
    "            sample_removal_targets = removal_targets[sample_idx]\n",
    "            \n",
    "            # Make hooks\n",
    "            net_parser = parser(net, param_ratio)\n",
    "            net_parser.register_hooks()\n",
    "\n",
    "            if isinstance(net_parser, selection.TopNGradients):\n",
    "                # Compute target loss\n",
    "                target_loss = (\n",
    "                    criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                    * len(removal_inputs)\n",
    "                    / (len(train_loader.dataset) - len(removal_inputs))\n",
    "                )\n",
    "                target_loss.backward()\n",
    "                net_parser.remove_hooks()\n",
    "\n",
    "            target_loss = (\n",
    "                criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                * len(removal_inputs)\n",
    "                / (len(train_loader.dataset) - len(removal_inputs))\n",
    "            )\n",
    "            \n",
    "            # Delete hooks\n",
    "            index_list = net_parser.get_parameters()\n",
    "            net_parser.remove_hooks()\n",
    "            \n",
    "            influence = hessians.generalized_influence(\n",
    "                net, total_loss, target_loss, index_list, tol=tol, step=3, max_iter=30, verbose=False\n",
    "            )\n",
    "            del target_loss, total_loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            influence = influence * 0.05 / torch.norm(influence)\n",
    "                \n",
    "            scale = 1\n",
    "            score = 0\n",
    "            best_score = -1\n",
    "            count = 1\n",
    "            save_path = (\n",
    "                f\"checkpoints/tab1/PIF/{net_name}/{net_parser.__class__.__name__}/{param_ratio}_{i}.pth\"\n",
    "            )\n",
    "            while True:\n",
    "                if score < .85:\n",
    "                    net_parser.update_network(influence * scale)\n",
    "                else:\n",
    "                    net_parser.update_network(influence * scale / 3)\n",
    "\n",
    "#                 self_loss, self_acc = test(net, test_loader, criterion, target_removal_label, True)\n",
    "#                 exclusive_loss, exclusive_acc = test(net, test_loader, criterion, target_removal_label, False)\n",
    "                self_loss, self_acc, exclusive_loss, exclusive_acc = influence_test(net, test_loader, \n",
    "                                                                                    criterion, target_removal_label)\n",
    "                score = f1_score(self_acc, exclusive_acc)\n",
    "                \n",
    "                if best_score < score:\n",
    "                    best_result = [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "                    best_score = score\n",
    "                    save_net(net, save_path)\n",
    "                    \n",
    "                if verbose:\n",
    "                    print(\n",
    "                    f\"{count} - test acc: {exclusive_acc:2.2f}, test loss: {exclusive_loss:.4f} |\" +\n",
    "                    f\" self-acc: {self_acc:2.2f}%, self loss: {self_loss:.4f} | score: {score:.7f}\",\n",
    "                    end='\\r'\n",
    "                    )\n",
    "                \n",
    "                if exclusive_acc < 80 or self_acc < 0.01 or count >= 200:\n",
    "                    if i == 0:\n",
    "                        result_list_TopNActivations += best_result\n",
    "                    elif i == 1:\n",
    "                        result_list_TopNGradients += best_result\n",
    "                    elif i == 2:\n",
    "                        result_list_Random += best_result\n",
    "                    else:\n",
    "                        result_list_Threshold += best_result\n",
    "                    \n",
    "                    print(f\"test acc: {best_result[0]:2.2f}, test loss: {best_result[1]:.4f} | \" +\n",
    "                          f\"self-acc: {best_result[2]:2.2f}%, self loss: {best_result[3]:.4f} | \" +\n",
    "                          f\"Score: {best_result[4]:.7f}\" + \" \" * 20) \n",
    "                    break\n",
    "                elif count >= 20 and best_score < 0.2:\n",
    "                    scale *= 5\n",
    "                elif count >= 50 and best_score < 0.5:\n",
    "                    print(f\"test acc: {best_result[0]:2.2f}, test loss: {best_result[1]:.4f} | \" +\n",
    "                          f\"self-acc: {best_result[2]:2.2f}%, self loss: {best_result[3]:.4f} | \" +\n",
    "                          f\"Score: {best_result[4]:.7f}\" + \" \" * 20) \n",
    "                    break\n",
    "\n",
    "                count += 1\n",
    "                \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "retrained_net = VGG11().to(device)\n",
    "net_name = retrained_net.__class__.__name__\n",
    "net_path = f\"../checkpoints/Figure_4/{net_name}/cross_entropy/ckpt_0.0_retrained.pth\"\n",
    "retrained_net = load_net(retrained_net, net_path)\n",
    "flatten = False\n",
    "\n",
    "loss, acc = test(retrained_net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")\n",
    "self_loss, self_acc = test(retrained_net, test_loader, criterion, 8, True)\n",
    "exclusive_loss, exclusive_acc = test(retrained_net, test_loader, criterion, 8, False)\n",
    "print(\n",
    "    f\"Retrained model \\t Self: {self_loss:.2f} {self_acc:2.2f}% | Exclusive loss: {exclusive_loss:.2f}, {exclusive_acc:2.2f}%\"\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
