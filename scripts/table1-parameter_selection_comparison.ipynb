{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb71cbff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0f03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import FullyConnectedNet, TinyNet, ResNet18\n",
    "from src import hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "target_removal_label = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "def test(net, dataloader, criterion, label, include):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        net_loss = 0\n",
    "        correct = 0\n",
    "        num_inputs = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            if include:\n",
    "                idx = targets == label\n",
    "            else:\n",
    "                idx = targets != label\n",
    "\n",
    "            inputs, targets = inputs[idx], targets[idx]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss * len(inputs)\n",
    "            num_inputs += targets.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / num_inputs * 100\n",
    "        net_loss /= num_inputs\n",
    "        return net_loss, accuracy\n",
    "    \n",
    "def influence_test(net, dataloader, criterion, removal_label):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        remain_loss = 0\n",
    "        remain_correct = 0\n",
    "        num_remain_inputs = 0\n",
    "        \n",
    "        removal_loss = 0\n",
    "        removal_correct = 0\n",
    "        num_removal_inputs = 0\n",
    "\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            removal_index = (targets == removal_label)\n",
    "\n",
    "            remain_inputs, remain_targets = inputs[~removal_index], targets[~removal_index]\n",
    "            remain_inputs, remain_targets = remain_inputs.to(device), remain_targets.to(device)\n",
    "            outputs = net(remain_inputs)\n",
    "            loss = criterion(outputs, remain_targets)\n",
    "            remain_loss += loss * len(remain_inputs)\n",
    "            num_remain_inputs += len(remain_inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            remain_correct += predicted.eq(remain_targets).sum().item()\n",
    "\n",
    "            removal_inputs, removal_targets = inputs[removal_index], targets[removal_index]\n",
    "            removal_inputs, removal_targets = removal_inputs.to(device), removal_targets.to(device)\n",
    "            outputs = net(removal_inputs)\n",
    "            loss = criterion(outputs, removal_targets)\n",
    "            removal_loss += loss * len(removal_inputs)\n",
    "            num_removal_inputs += len(removal_inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            removal_correct += predicted.eq(removal_targets).sum().item()\n",
    "            \n",
    "        remain_accuracy = remain_correct / num_remain_inputs * 100\n",
    "        remain_loss /= num_remain_inputs\n",
    "\n",
    "        removal_accuracy = removal_correct / num_removal_inputs * 100\n",
    "        removal_loss /= num_removal_inputs\n",
    "        \n",
    "        return remain_accuracy, remain_loss, removal_accuracy, removal_loss\n",
    "    \n",
    "def sample_test(net, criterion, inputs, targets):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / len(inputs) * 100\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "def f1_score(test_acc, self_acc):\n",
    "    self_acc /= 100\n",
    "    test_acc /= 100\n",
    "    return 2 * (1 - self_acc) * test_acc / (1.00001 - self_acc + test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f993d",
   "metadata": {},
   "source": [
    "### Building model and set criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96121842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet finished. \n",
      "    Number of parameters: 11172810\n",
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "net = ResNet18(1).to(device)\n",
    "net_name = \"ResNet18\"\n",
    "net_path = f\"checkpoints/tab2/{net_name}/cross_entropy/ckpt_0.0.pth\"\n",
    "\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net_name = net.__class__.__name__\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 256\n",
    "num_workers = 24\n",
    "num_sample_batch = 1\n",
    "num_target_sample = 512\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, validation=False)\n",
    "train_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "# loss, acc = test(net, test_loader, criterion)\n",
    "# print(\n",
    "#     f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af664a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "removal_inputs = list()\n",
    "removal_targets = list()\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    idx = targets == target_removal_label\n",
    "    removal_inputs.append(inputs[idx])\n",
    "    removal_targets.append(targets[idx])\n",
    "removal_inputs = torch.cat(removal_inputs)\n",
    "removal_targets = torch.cat(removal_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84190faf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser: TopNActivations, param_ratio: 10%\n",
      "6 - test acc: 99.22, test loss: 0.0256 | self-acc: 99.28%, self loss: 0.0202 | Score: 0.0143\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 74\u001b[0m\n\u001b[1;32m     70\u001b[0m                 utils\u001b[38;5;241m.\u001b[39mupdate_network(net, influence \u001b[38;5;241m*\u001b[39m scale, index_list)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#                 self_loss, self_acc = test(net, test_loader, criterion, target_removal_label, True)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#                 exclusive_loss, exclusive_acc = test(net, test_loader, criterion, target_removal_label, False)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m                 exclusive_acc, exclusive_loss, self_acc, self_loss \u001b[38;5;241m=\u001b[39m \u001b[43minfluence_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                                                                    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_removal_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m                 score \u001b[38;5;241m=\u001b[39m f1_score(exclusive_acc, self_acc)\n\u001b[1;32m     78\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[3], line 65\u001b[0m, in \u001b[0;36minfluence_test\u001b[0;34m(net, dataloader, criterion, removal_label)\u001b[0m\n\u001b[1;32m     63\u001b[0m num_remain_inputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(remain_inputs)\n\u001b[1;32m     64\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m remain_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremain_targets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m removal_inputs, removal_targets \u001b[38;5;241m=\u001b[39m inputs[removal_index], targets[removal_index]\n\u001b[1;32m     68\u001b[0m removal_inputs, removal_targets \u001b[38;5;241m=\u001b[39m removal_inputs\u001b[38;5;241m.\u001b[39mto(device), removal_targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser_list = [selection.TopNActivations,\n",
    "               selection.TopNGradients,\n",
    "               selection.Random,\n",
    "               selection.Threshold,]\n",
    "\n",
    "ratio_list = [10, 30, 50, 100]\n",
    "tol = 1e-9\n",
    "\n",
    "for i in range(10):\n",
    "    # Sampling the target removal data\n",
    "    sample_idx = np.random.choice(len(removal_inputs), num_target_sample, replace=False)\n",
    "    \n",
    "    for parser in parser_list:\n",
    "        for param_ratio in ratio_list:                      \n",
    "            # Initialize network\n",
    "            net = ResNet18(1).to(device)\n",
    "            net = load_net(net, net_path)\n",
    "            net_parser = parser(net, param_ratio/100)\n",
    "            print(f\"Parser: {net_parser.__class__.__name__}, param_ratio: {param_ratio}%\")\n",
    "            \n",
    "            # Compute total loss\n",
    "            total_loss = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                if batch_idx >= num_sample_batch:\n",
    "                    break\n",
    "                idx = targets != target_removal_label\n",
    "                inputs, targets = inputs[idx], targets[idx]\n",
    "                total_loss += criterion(net(inputs.to(device)), targets.to(device))\n",
    "            total_loss /= num_sample_batch\n",
    "\n",
    "            # Register hook to select the parameters\n",
    "            net_parser.register_hooks()\n",
    "\n",
    "            # Compute target loss\n",
    "            sample_removal_inputs = removal_inputs[sample_idx]\n",
    "            sample_removal_targets = removal_targets[sample_idx]\n",
    "            target_loss = (\n",
    "                criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                * len(removal_inputs) / len(train_loader.dataset)\n",
    "            )\n",
    "            \n",
    "            # Exception handling for backward hook\n",
    "            if isinstance(net_parser, selection.TopNGradients):\n",
    "                target_loss.backward()\n",
    "                net_parser.remove_hooks()\n",
    "                target_loss = (\n",
    "                criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                * len(removal_inputs) / len(train_loader.dataset)\n",
    "                )\n",
    "\n",
    "            # Get index list to compute GIF\n",
    "            index_list = net_parser.get_parameters()\n",
    "            net_parser.remove_hooks()\n",
    "\n",
    "            influence = hessians.generalized_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol=tol, step=3, max_iter=30, verbose=False\n",
    "            )\n",
    "            del total_loss, target_loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            scale = 1\n",
    "            count = 1\n",
    "            best_score = -1\n",
    "            save_path = (\n",
    "                f\"checkpoints/tab1/PIF/{net_name}/{net_parser.__class__.__name__}/{param_ratio}_{i}.pth\"\n",
    "            )\n",
    "            while True:\n",
    "#                 net_parser.update_network(influence * scale)\n",
    "                utils.update_network(net, influence * scale, index_list)\n",
    "                \n",
    "#                 self_loss, self_acc = test(net, test_loader, criterion, target_removal_label, True)\n",
    "#                 exclusive_loss, exclusive_acc = test(net, test_loader, criterion, target_removal_label, False)\n",
    "                exclusive_acc, exclusive_loss, self_acc, self_loss = influence_test(net, test_loader, \n",
    "                                                                                    criterion, target_removal_label)\n",
    "                score = f1_score(exclusive_acc, self_acc)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\n",
    "                    f\"{count} - test acc: {exclusive_acc:2.2f}, test loss: {exclusive_loss:.4f}\" + \\\n",
    "                    f\" | self-acc: {self_acc:2.2f}%, self loss: {self_loss:.4f} | Score: {score:.4f}\",\n",
    "                    end = '\\r'\n",
    "                    ) \n",
    "                    \n",
    "                if best_score < score:\n",
    "                    best_result = [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "                    best_score = score\n",
    "                    save_net(net, save_path)\n",
    "\n",
    "                if exclusive_acc < 75 or self_acc < 0.1 or count > 200:\n",
    "                    print(f\"test acc: {best_result[0]:2.2f}, test loss: {best_result[1]:.4f} | \" +\n",
    "                          f\"self-acc: {best_result[2]:2.2f}%, self loss: {best_result[3]:.4f} | \" +\n",
    "                          f\"Score: {best_result[4]:.7f} \\n\") \n",
    "                    break\n",
    "\n",
    "                count += 1\n",
    "            del net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92865",
   "metadata": {},
   "source": [
    "### Measure the network utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "\n",
    "# Define lists to contain results\n",
    "self_loss_list = [[],[],[],[]]\n",
    "self_acc_list = [[],[],[],[]]\n",
    "exclusive_loss_list = [[],[],[],[]]\n",
    "exclusive_acc_list = [[],[],[],[]]\n",
    "\n",
    "parser_count = 0\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        _, _, test_loader = data_loader.get_data_loaders()\n",
    "        param_ratio *= 0.01\n",
    "        \n",
    "        net_path = f\"checkpoints/tab1/PIF/ResNet/{net_parser.__class__.__name__}/{param_ratio}_1.pth\"\n",
    "        net = load_net(net, net_path)\n",
    "\n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        self_loss_list[parser_count].append(self_loss.detach().cpu())\n",
    "        self_acc_list[parser_count].append(self_acc)\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "\n",
    "        # Save results in defined lists\n",
    "        exclusive_loss_list[parser_count].append(exclusive_loss.detach().cpu())\n",
    "        exclusive_acc_list[parser_count].append(exclusive_acc)\n",
    "\n",
    "        print(f\"{net_parser.__class__.__name__}, {param_ratio*100:2.0f}% - Self: {self_loss:.4f} {self_acc:.2f}% | exclusive loss: {exclusive_loss:.4f}, {exclusive_acc:.2f}%\")\n",
    "        print(\"\")\n",
    "    parser_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Show results\n",
    "print(\"Self Loss\")\n",
    "for i in range(4):\n",
    "    self_loss_list[i] = [float(tensor) for tensor in self_loss_list[i]]\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in self_loss_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in self_loss_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in self_loss_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in self_loss_list[3]],\n",
    "       }\n",
    "self_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_loss_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "self_loss_df = self_loss_df.transpose()\n",
    "print(self_loss_df)\n",
    "\n",
    "print(\"\\nSelf Accuracy\")\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in self_acc_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in self_acc_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in self_acc_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in self_acc_list[3]]\n",
    "       }\n",
    "self_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_acc_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "self_acc_df = self_acc_df.transpose()\n",
    "print(self_acc_df)\n",
    "\n",
    "for i in range(4):\n",
    "    exclusive_loss_list[i] = [float(tensor) for tensor in exclusive_loss_list[i]]\n",
    "print(\"\\nExclusive Loss\")\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[3]],\n",
    "       }\n",
    "exclusive_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_loss_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "exclusive_loss_df = exclusive_loss_df.transpose()\n",
    "print(exclusive_loss_df)\n",
    "\n",
    "print(\"\\nExclusive Accuracy\")\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[3]],\n",
    "       }\n",
    "exclusive_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_acc_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "exclusive_acc_df = exclusive_acc_df.transpose()\n",
    "print(exclusive_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save list files of results\n",
    "with open('self_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_acc, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('exclusive_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_acc, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262802f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
