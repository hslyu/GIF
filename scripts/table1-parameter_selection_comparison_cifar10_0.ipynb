{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caff5365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cbc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import cifar10\n",
    "from models import VGG11\n",
    "from src import freeze_influence, hessians, selection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "target_removal_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4643ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def test(net, dataloader, criterion, label, include):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        net_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_data = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            if include:\n",
    "                idx = targets == label\n",
    "            else:\n",
    "                idx = targets != label\n",
    "            inputs = inputs[idx]\n",
    "            targets = targets[idx]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss * len(inputs)\n",
    "            num_data +=  len(inputs)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total * 100\n",
    "        net_loss /= num_data\n",
    "        return net_loss, accuracy\n",
    "\n",
    "def influence_test(net, dataloader, criterion, target_label):\n",
    "    def sample_test(net, criterion, inputs, targets):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct = predicted.eq(targets).sum().item()\n",
    "\n",
    "            return loss, correct\n",
    "\n",
    "    self_loss = 0\n",
    "    self_correct = 0\n",
    "    num_self_inputs = 0\n",
    "    \n",
    "    exclusive_loss = 0\n",
    "    exclusive_correct = 0\n",
    "    num_exclusive_inputs = 0\n",
    "    \n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        target_idx = (targets == target_label)\n",
    "        batch_self_loss, batch_self_correct = sample_test(net, criterion, inputs[target_idx], targets[target_idx])\n",
    "        batch_exclusive_loss, batch_exclusive_correct = sample_test(net, criterion, inputs[~target_idx], targets[~target_idx])\n",
    "        \n",
    "        len_self_batch = len(inputs[target_idx])\n",
    "        self_loss += batch_self_loss * len_self_batch\n",
    "        self_correct += batch_self_correct\n",
    "        num_self_inputs += len_self_batch\n",
    "        \n",
    "        len_exclusive_batch = len(inputs[~target_idx])\n",
    "        exclusive_loss += batch_exclusive_loss * len_exclusive_batch\n",
    "        exclusive_correct += batch_exclusive_correct\n",
    "        num_exclusive_inputs += len_exclusive_batch\n",
    "        \n",
    "    self_loss /= num_self_inputs\n",
    "    self_acc = self_correct / num_self_inputs * 100\n",
    "    exclusive_loss /= num_exclusive_inputs\n",
    "    exclusive_acc = exclusive_correct / num_exclusive_inputs * 100\n",
    "    \n",
    "    return self_loss, self_acc, exclusive_loss, exclusive_acc\n",
    "\n",
    "def projected_influence(net, total_loss, target_loss, index_list, tol, step, max_iter, verbose):\n",
    "    num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    full_param_index_list = np.arange(num_param)\n",
    "    influence = hessians.generalized_influence(\n",
    "        net, total_loss, target_loss, full_param_index_list, tol=tol, step=step, max_iter=max_iter, verbose=verbose\n",
    "    )\n",
    "    return influence[idx]\n",
    "\n",
    "def f1_score(self_acc, test_acc):\n",
    "    self_acc /= 100\n",
    "    test_acc /= 100\n",
    "    if self_acc == 1 and test_acc == 0:\n",
    "        return 0\n",
    "    return 2 * (1 - self_acc) * test_acc / (1 - self_acc + test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d193c3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building VGG11 finished. \n",
      "    Number of parameters: 9231114\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Original loss and acc : 0.3518, 91.50%\n"
     ]
    }
   ],
   "source": [
    "net = VGG11().to(device)\n",
    "net_name = \"VGG11\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "net_path = f\"checkpoints/tab2/{net_name}/cross_entropy/ckpt_0.0.pth\"\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 16\n",
    "num_sample_batch = 1\n",
    "num_target_sample = 512\n",
    "\n",
    "data_loader = cifar10.CIFAR10DataLoader(batch_size, num_workers, validation=False)\n",
    "train_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "loss, acc = test(net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb212049",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "num_exp = 10\n",
    "\n",
    "removal_inputs = list()\n",
    "removal_targets = list()\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    idx = targets == target_removal_label\n",
    "    removal_inputs.append(inputs[idx])\n",
    "    removal_targets.append(targets[idx])\n",
    "removal_inputs = torch.cat(removal_inputs)\n",
    "removal_targets = torch.cat(removal_targets)\n",
    "\n",
    "ratio_list = [.1, .3, .5]\n",
    "result_list_TopNActivations = []\n",
    "result_list_TopNGradients = []\n",
    "result_list_Random = []\n",
    "result_list_Threshold = []\n",
    "tol = 1e-9\n",
    "\n",
    "parser_list = [selection.TopNActivations,\n",
    "              selection.TopNGradients,\n",
    "              selection.Random,\n",
    "              selection.Threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e343f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopNActivations - ratio: 10.0%\n",
      "test acc: 85.31, test loss: 0.6452 | self-acc: 3.80%, self loss: 8.0423 | Score: 0.9042894                    \n",
      "TopNGradients - ratio: 10.0%\n",
      "test acc: 86.79, test loss: 0.5534 | self-acc: 2.60%, self loss: 9.1917 | Score: 0.9178879                    \n",
      "Random - ratio: 10.0%\n",
      "test acc: 83.28, test loss: 0.7191 | self-acc: 3.30%, self loss: 8.6100 | Score: 0.8948839                    \n",
      "Threshold - ratio: 10.0%\n",
      "test acc: 80.39, test loss: 0.7918 | self-acc: 3.90%, self loss: 8.0176 | Score: 0.8754514                    \n",
      "\n",
      "TopNActivations - ratio: 30.0%\n",
      "test acc: 83.79, test loss: 0.7312 | self-acc: 4.30%, self loss: 9.1405 | Score: 0.8934923                    \n",
      "TopNGradients - ratio: 30.0%\n",
      "test acc: 86.53, test loss: 0.5786 | self-acc: 1.90%, self loss: 9.5199 | Score: 0.9195436                    \n",
      "Random - ratio: 30.0%\n",
      "test acc: 85.18, test loss: 0.6667 | self-acc: 2.90%, self loss: 8.9237 | Score: 0.9074899                    \n",
      "Threshold - ratio: 30.0%\n",
      "test acc: 83.84, test loss: 0.6853 | self-acc: 3.60%, self loss: 8.0390 | Score: 0.8968492                    \n",
      "\n",
      "TopNActivations - ratio: 50.0%\n",
      "test acc: 85.11, test loss: 0.6613 | self-acc: 5.60%, self loss: 8.2333 | Score: 0.8951523                    \n",
      "TopNGradients - ratio: 50.0%\n",
      "test acc: 85.83, test loss: 0.6209 | self-acc: 1.70%, self loss: 9.6177 | Score: 0.9164464                    \n",
      "Random - ratio: 50.0%\n",
      "test acc: 85.66, test loss: 0.6446 | self-acc: 3.30%, self loss: 8.5485 | Score: 0.9084332                    \n",
      "Threshold - ratio: 50.0%\n",
      "test acc: 84.27, test loss: 0.6975 | self-acc: 3.90%, self loss: 8.5263 | Score: 0.8979516                    \n",
      "\n",
      "TopNActivations - ratio: 10.0%\n",
      "test acc: 86.07, test loss: 0.6089 | self-acc: 2.40%, self loss: 7.7229 | Score: 0.9147122                    \n",
      "TopNGradients - ratio: 10.0%\n",
      "test acc: 87.03, test loss: 0.5471 | self-acc: 1.20%, self loss: 8.7715 | Score: 0.9254414                    \n",
      "Random - ratio: 10.0%\n",
      "test acc: 84.26, test loss: 0.6825 | self-acc: 2.50%, self loss: 8.0437 | Score: 0.9039522                    \n",
      "Threshold - ratio: 10.0%\n",
      "test acc: 83.42, test loss: 0.6673 | self-acc: 5.80%, self loss: 6.9275 | Score: 0.8848412                    \n",
      "\n",
      "TopNActivations - ratio: 30.0%\n",
      "test acc: 83.22, test loss: 0.7452 | self-acc: 2.80%, self loss: 8.8424 | Score: 0.8966966                    \n",
      "TopNGradients - ratio: 30.0%\n",
      "test acc: 86.83, test loss: 0.5558 | self-acc: 2.00%, self loss: 8.6790 | Score: 0.9207935                    \n",
      "Random - ratio: 30.0%\n",
      "test acc: 85.02, test loss: 0.6706 | self-acc: 1.50%, self loss: 8.8206 | Score: 0.9126621                    \n",
      "Threshold - ratio: 30.0%\n",
      "test acc: 82.92, test loss: 0.7358 | self-acc: 3.40%, self loss: 8.0572 | Score: 0.8924006                    \n",
      "\n",
      "TopNActivations - ratio: 50.0%\n",
      "test acc: 82.89, test loss: 0.7725 | self-acc: 1.80%, self loss: 8.8747 | Score: 0.8989717                    \n",
      "TopNGradients - ratio: 50.0%\n",
      "test acc: 85.59, test loss: 0.6376 | self-acc: 0.90%, self loss: 9.2812 | Score: 0.9185023                    \n",
      "Random - ratio: 50.0%\n",
      "test acc: 84.90, test loss: 0.6686 | self-acc: 2.20%, self loss: 8.5691 | Score: 0.9089458                    \n",
      "Threshold - ratio: 50.0%\n",
      "test acc: 83.78, test loss: 0.7143 | self-acc: 1.80%, self loss: 8.2692 | Score: 0.9041739                    \n",
      "\n",
      "TopNActivations - ratio: 10.0%\n",
      "test acc: 85.14, test loss: 0.6145 | self-acc: 1.70%, self loss: 8.0743 | Score: 0.9125050                    \n",
      "TopNGradients - ratio: 10.0%\n",
      "test acc: 87.72, test loss: 0.4909 | self-acc: 1.20%, self loss: 8.6059 | Score: 0.9293215                    \n",
      "Random - ratio: 10.0%\n",
      "test acc: 84.24, test loss: 0.6382 | self-acc: 2.00%, self loss: 8.0056 | Score: 0.9060310                    \n",
      "Threshold - ratio: 10.0%\n",
      "test acc: 81.89, test loss: 0.7009 | self-acc: 4.20%, self loss: 7.2317 | Score: 0.8829990                    \n",
      "\n",
      "TopNActivations - ratio: 30.0%\n",
      "test acc: 83.14, test loss: 0.7122 | self-acc: 1.50%, self loss: 9.1102 | Score: 0.9017317                    \n",
      "TopNGradients - ratio: 30.0%\n",
      "test acc: 87.12, test loss: 0.5325 | self-acc: 1.90%, self loss: 8.5966 | Score: 0.9228579                    \n",
      "Random - ratio: 30.0%\n",
      "test acc: 85.82, test loss: 0.6029 | self-acc: 2.50%, self loss: 7.9254 | Score: 0.9128917                    \n",
      "Threshold - ratio: 30.0%\n",
      "test acc: 83.11, test loss: 0.6861 | self-acc: 3.00%, self loss: 7.8957 | Score: 0.8952005                    \n",
      "\n",
      "TopNActivations - ratio: 50.0%\n",
      "test acc: 83.13, test loss: 0.7186 | self-acc: 1.80%, self loss: 8.6636 | Score: 0.9004074                    \n",
      "TopNGradients - ratio: 50.0%\n",
      "test acc: 86.41, test loss: 0.5730 | self-acc: 1.60%, self loss: 8.7036 | Score: 0.9201669                    \n",
      "Random - ratio: 50.0%\n",
      "test acc: 85.30, test loss: 0.6363 | self-acc: 2.30%, self loss: 8.1052 | Score: 0.9107989                    \n",
      "Threshold - ratio: 50.0%\n",
      "test acc: 83.86, test loss: 0.6812 | self-acc: 3.00%, self loss: 8.1285 | Score: 0.8995011                    \n",
      "\n",
      "TopNActivations - ratio: 10.0%\n",
      "test acc: 86.90, test loss: 0.5308 | self-acc: 0.90%, self loss: 7.8090 | Score: 0.9259989                    \n",
      "TopNGradients - ratio: 10.0%\n",
      "test acc: 88.36, test loss: 0.4754 | self-acc: 0.70%, self loss: 8.5792 | Score: 0.9350863                    \n",
      "Random - ratio: 10.0%\n",
      "test acc: 86.87, test loss: 0.5477 | self-acc: 1.60%, self loss: 8.0129 | Score: 0.9227434                    \n",
      "Threshold - ratio: 10.0%\n",
      "test acc: 83.37, test loss: 0.6543 | self-acc: 3.00%, self loss: 8.0759 | Score: 0.8966808                    \n",
      "\n",
      "TopNActivations - ratio: 30.0%\n",
      "test acc: 85.86, test loss: 0.5824 | self-acc: 1.80%, self loss: 8.3456 | Score: 0.9161381                    \n",
      "TopNGradients - ratio: 30.0%\n",
      "test acc: 87.63, test loss: 0.5039 | self-acc: 0.60%, self loss: 8.8646 | Score: 0.9314653                    \n",
      "Random - ratio: 30.0%\n",
      "test acc: 86.78, test loss: 0.5590 | self-acc: 2.50%, self loss: 7.9630 | Score: 0.9182695                    \n",
      "Threshold - ratio: 30.0%\n",
      "test acc: 84.88, test loss: 0.6021 | self-acc: 2.20%, self loss: 7.4282 | Score: 0.9088184                    \n",
      "\n",
      "TopNActivations - ratio: 50.0%\n",
      "test acc: 85.33, test loss: 0.6150 | self-acc: 2.20%, self loss: 8.2089 | Score: 0.9114234                    \n",
      "TopNGradients - ratio: 50.0%\n",
      "test acc: 87.57, test loss: 0.5233 | self-acc: 1.20%, self loss: 8.6153 | Score: 0.9284479                    \n",
      "Random - ratio: 50.0%\n",
      "test acc: 85.99, test loss: 0.6011 | self-acc: 1.00%, self loss: 8.6131 | Score: 0.9203688                    \n",
      "Threshold - ratio: 50.0%\n",
      "test acc: 85.81, test loss: 0.5927 | self-acc: 1.40%, self loss: 8.0172 | Score: 0.9176210                    \n",
      "\n",
      "TopNActivations - ratio: 10.0%\n",
      "test acc: 86.63, test loss: 0.5472 | self-acc: 1.60%, self loss: 7.1909 | Score: 0.9214253                    \n",
      "TopNGradients - ratio: 10.0%\n",
      "test acc: 87.88, test loss: 0.4855 | self-acc: 0.90%, self loss: 8.0167 | Score: 0.9315212                    \n",
      "Random - ratio: 10.0%\n",
      "test acc: 83.90, test loss: 0.6700 | self-acc: 1.80%, self loss: 7.3666 | Score: 0.9048852                    \n",
      "Threshold - ratio: 10.0%\n",
      "test acc: 82.06, test loss: 0.6886 | self-acc: 3.60%, self loss: 7.0313 | Score: 0.8865127                    \n",
      "\n",
      "TopNActivations - ratio: 30.0%\n",
      "test acc: 82.66, test loss: 0.7210 | self-acc: 1.90%, self loss: 8.1243 | Score: 0.8971796                    \n",
      "TopNGradients - ratio: 30.0%\n",
      "test acc: 86.37, test loss: 0.5509 | self-acc: 0.70%, self loss: 8.6505 | Score: 0.9238287                    \n",
      "Random - ratio: 30.0%\n",
      "test acc: 84.70, test loss: 0.6398 | self-acc: 1.90%, self loss: 7.9078 | Score: 0.9090886                    \n",
      "Threshold - ratio: 30.0%\n",
      "test acc: 83.26, test loss: 0.6916 | self-acc: 2.30%, self loss: 7.5321 | Score: 0.8990128                    \n",
      "\n",
      "TopNActivations - ratio: 50.0%\n",
      "test acc: 83.04, test loss: 0.7187 | self-acc: 2.50%, self loss: 7.9935 | Score: 0.8969352                    \n",
      "TopNGradients - ratio: 50.0%\n",
      "test acc: 85.94, test loss: 0.6011 | self-acc: 2.30%, self loss: 8.1079 | Score: 0.9144597                    \n",
      "Random - ratio: 50.0%\n",
      "test acc: 84.23, test loss: 0.6725 | self-acc: 1.80%, self loss: 8.2182 | Score: 0.9068204                    \n",
      "Threshold - ratio: 50.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 84.36, test loss: 0.6636 | self-acc: 3.50%, self loss: 7.5278 | Score: 0.9002003                    \n",
      "\n",
      "TopNActivations - ratio: 10.0%\n",
      "test acc: 86.00, test loss: 0.5805 | self-acc: 1.40%, self loss: 7.8957 | Score: 0.9186999                    \n",
      "TopNGradients - ratio: 10.0%\n",
      "test acc: 87.50, test loss: 0.5143 | self-acc: 0.40%, self loss: 8.6690 | Score: 0.9315874                    \n",
      "Random - ratio: 10.0%\n",
      "34 - test acc: 80.62, test loss: 0.8168 | self-acc: 0.40%, self loss: 8.8328 | score: 0.89111918\r"
     ]
    }
   ],
   "source": [
    "for i in range(num_exp):\n",
    "    sample_idx = np.random.choice(len(removal_inputs), num_target_sample, replace=False)\n",
    "    for param_ratio in ratio_list:\n",
    "        for parser in parser_list:\n",
    "\n",
    "            print(f\"{parser.__name__} - ratio: {param_ratio*100}%\")\n",
    "            # Initialize network\n",
    "            net = load_net(net, net_path)\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "                if batch_idx >= num_sample_batch:\n",
    "                    break\n",
    "                idx = targets != target_removal_label\n",
    "                inputs, targets = inputs[idx], targets[idx]\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                total_loss += criterion(outputs, targets)\n",
    "\n",
    "            # Sampling the target removal data\n",
    "            sample_removal_inputs = removal_inputs[sample_idx]\n",
    "            sample_removal_targets = removal_targets[sample_idx]\n",
    "            \n",
    "            # Make hooks\n",
    "            net_parser = parser(net, param_ratio)\n",
    "            net_parser.register_hooks()\n",
    "\n",
    "            if isinstance(net_parser, selection.TopNGradients):\n",
    "                # Compute target loss\n",
    "                target_loss = (\n",
    "                    criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                    * len(removal_inputs)\n",
    "                    / (len(train_loader.dataset) - len(removal_inputs))\n",
    "                )\n",
    "                target_loss.backward()\n",
    "                net_parser.remove_hooks()\n",
    "\n",
    "            target_loss = (\n",
    "                criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                * len(removal_inputs)\n",
    "                / (len(train_loader.dataset) - len(removal_inputs))\n",
    "            )\n",
    "            \n",
    "            # Delete hooks\n",
    "            index_list = net_parser.get_parameters()\n",
    "            net_parser.remove_hooks()\n",
    "            \n",
    "            influence = hessians.generalized_influence(\n",
    "                net, total_loss, target_loss, index_list, tol=tol, step=3, max_iter=30, verbose=False\n",
    "            )\n",
    "            del target_loss, total_loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            influence = influence * 0.05 / torch.norm(influence)\n",
    "                \n",
    "            scale = 1\n",
    "            score = 0\n",
    "            best_score = -1\n",
    "            count = 1\n",
    "            save_path = (\n",
    "                f\"checkpoints/tab1/{net_name}/{net_parser.__class__.__name__}/{param_ratio}_{i}.pth\"\n",
    "            )\n",
    "            while True:\n",
    "                if score < .85:\n",
    "                    net_parser.update_network(influence * scale)\n",
    "                else:\n",
    "                    net_parser.update_network(influence * scale / 3)\n",
    "\n",
    "#                 self_loss, self_acc = test(net, test_loader, criterion, target_removal_label, True)\n",
    "#                 exclusive_loss, exclusive_acc = test(net, test_loader, criterion, target_removal_label, False)\n",
    "                self_loss, self_acc, exclusive_loss, exclusive_acc = influence_test(net, test_loader, \n",
    "                                                                                    criterion, target_removal_label)\n",
    "                score = f1_score(self_acc, exclusive_acc)\n",
    "                \n",
    "                if best_score < score:\n",
    "                    best_result = [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "                    best_score = score\n",
    "                    save_net(net, save_path)\n",
    "                    \n",
    "                if verbose:\n",
    "                    print(\n",
    "                    f\"{count} - test acc: {exclusive_acc:2.2f}, test loss: {exclusive_loss:.4f} |\" +\n",
    "                    f\" self-acc: {self_acc:2.2f}%, self loss: {self_loss:.4f} | score: {score:.7f}\",\n",
    "                    end='\\r'\n",
    "                    )\n",
    "                \n",
    "                if exclusive_acc < 80 or self_acc < 0.01 or count >= 200:\n",
    "                    if i == 0:\n",
    "                        result_list_TopNActivations += best_result\n",
    "                    elif i == 1:\n",
    "                        result_list_TopNGradients += best_result\n",
    "                    elif i == 2:\n",
    "                        result_list_Random += best_result\n",
    "                    else:\n",
    "                        result_list_Threshold += best_result\n",
    "                    \n",
    "                    print(f\"test acc: {best_result[0]:2.2f}, test loss: {best_result[1]:.4f} | \" +\n",
    "                          f\"self-acc: {best_result[2]:2.2f}%, self loss: {best_result[3]:.4f} | \" +\n",
    "                          f\"Score: {best_result[4]:.7f}\" + \" \" * 20) \n",
    "                    break\n",
    "                elif count >= 20 and best_score < 0.2:\n",
    "                    scale *= 5\n",
    "                elif count >= 50 and best_score < 0.5:\n",
    "                    print(f\"test acc: {best_result[0]:2.2f}, test loss: {best_result[1]:.4f} | \" +\n",
    "                          f\"self-acc: {best_result[2]:2.2f}%, self loss: {best_result[3]:.4f} | \" +\n",
    "                          f\"Score: {best_result[4]:.7f}\" + \" \" * 20) \n",
    "                    break\n",
    "\n",
    "                count += 1\n",
    "                \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_ratio in ratio_list:\n",
    "    print(\"\")\n",
    "    for parser in parser_list:\n",
    "        print(f\"{parser.__name__} - ratio: {param_ratio*100}%\")\n",
    "        \n",
    "        self_loss_list = np.empty(0)\n",
    "        self_acc_list = np.empty(0)\n",
    "        exclusive_loss_list = np.empty(0)\n",
    "        exclusive_acc_list = np.empty(0)\n",
    "        \n",
    "        for i in range(num_exp):\n",
    "\n",
    "            load_path = (\n",
    "                f\"checkpoints/tab1/{net_name}/{parser.__name__}/{param_ratio}_{i}.pth\"\n",
    "            )\n",
    "            net = VGG11().to(device)\n",
    "            net = load_net(net, load_path)\n",
    "            self_loss, self_acc, exclusive_loss, exclusive_acc = influence_test(net, test_loader, \n",
    "                                                                    criterion, target_removal_label)\n",
    "            \n",
    "            self_loss_list = np.append(self_loss_list, self_loss.detach().cpu().numpy())\n",
    "            self_acc_list = np.append(self_acc_list, self_acc)\n",
    "            exclusive_loss_list = np.append(exclusive_loss_list, exclusive_loss.detach().cpu().numpy())\n",
    "            exclusive_acc_list = np.append(exclusive_acc_list, exclusive_acc)\n",
    "            print(\n",
    "            f\"  test acc: {exclusive_acc:2.2f}, test loss: {exclusive_loss:.4f} | \" +\n",
    "            f\"self-acc: {self_acc:2.2f}%, self loss: {self_loss:.4f}\",\n",
    "            end='\\r'\n",
    "            )\n",
    "            \n",
    "        mean_self_loss = np.mean(self_loss_list)\n",
    "        mean_self_acc = np.mean(self_acc_list)\n",
    "        mean_exclusive_loss = np.mean(exclusive_loss_list)\n",
    "        mean_exclusive_acc = np.mean(exclusive_acc_list)\n",
    "        mean_f1_score = np.mean(f1_score_list)\n",
    "                \n",
    "        var_self_loss = np.var(self_loss_list)\n",
    "        var_self_acc = np.var(self_acc_list)\n",
    "        var_exclusive_loss = np.var(exclusive_loss_list)\n",
    "        var_exclusive_acc = np.var(exclusive_acc_list)\n",
    "        var_f1_score = np.var(f1_score_list)\n",
    "\n",
    "        print(\n",
    "        f\"{mean_exclusive_acc:2.2f} $\\pm$ {var_exclusive_acc:2.2f} & \" +\n",
    "        f\"{mean_exclusive_loss:.4f} $\\pm$ {var_exclusive_loss:.4f} & \", end=\"\"\n",
    "        )\n",
    "        print(\n",
    "        f\"{mean_self_acc:2.2f} $\\pm$ {var_self_acc:2.2f} & \" +\n",
    "        f\"{mean_self_loss:.4f} $\\pm$ {var_self_loss:.4f} \"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
