{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import FullyConnectedNet, TinyNet, ResNet18\n",
    "from src import hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e7aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(net, dataloader, criterion, num_batch_sample: int=-1):\n",
    "    net_loss = 0\n",
    "    num_batch_sample = len(dataloader) if num_batch_sample == -1 else num_batch_sample\n",
    "    sample_indices = np.random.choice(len(dataloader), size=num_batch_sample, replace=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if batch_idx in sample_indices:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss\n",
    "\n",
    "    net_loss /= num_batch_sample\n",
    "    return net_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f993d",
   "metadata": {},
   "source": [
    "### Building model and set criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96121842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet finished. \n",
      "    Number of parameters: 11172810\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "net = ResNet18(1).to(device)\n",
    "flatten = False\n",
    "net_path = \"../checkpoints/Figure_3/ResNet/cross_entropy/ckpt_0.0.pth\"\n",
    "\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "net_name = net.__class__.__name__\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5ef1c",
   "metadata": {},
   "source": [
    "### Preparing data and register hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b393bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, flatten=flatten)\n",
    "train_loader, val_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "data = list()\n",
    "target = list()\n",
    "for batch_idx, (data_raw, target_raw) in enumerate(train_loader):\n",
    "    idx = target_raw == 8\n",
    "    data_raw = data_raw[idx]\n",
    "    target_raw = target_raw[idx]\n",
    "    data.append(data_raw)\n",
    "    target.append(target_raw)\n",
    "data = torch.cat(data)\n",
    "target = torch.cat(target)\n",
    "sample_idx = np.random.choice(len(data), 50, replace=False)\n",
    "sample_data = data[sample_idx]\n",
    "sample_target = target[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b2bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Define hooks\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Define hooks\")\n",
    "# Make hooks\n",
    "parser_list = [selection.TopNActivations, \n",
    "                  selection.TopNGradients,\n",
    "                  selection.RandomSelection,\n",
    "                  selection.Threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928c281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing influence..\n",
      "Parser: TopNActivations, param_ratio: 5%\n",
      "Computing partial influence ... [230/10000], Tolerance: 9.998E-07, Avg. computing time: 0.558s          \n",
      "Parser: TopNActivations, param_ratio: 10%\n",
      "Computing partial influence ... [11/10000], Tolerance: 9.059E-07, Avg. computing time: 0.545s          \n",
      "Parser: TopNActivations, param_ratio: 15%\n",
      "Computing partial influence ... [7/10000], Tolerance: 9.207E-07, Avg. computing time: 0.545s          \n",
      "Parser: TopNActivations, param_ratio: 20%\n",
      "Computing partial influence ... [53/10000], Tolerance: 9.890E-07, Avg. computing time: 0.544s          \n",
      "Parser: TopNActivations, param_ratio: 25%\n",
      "Computing partial influence ... [7/10000], Tolerance: 9.209E-07, Avg. computing time: 0.537s          \n",
      "Parser: TopNActivations, param_ratio: 30%\n",
      "Computing partial influence ... [21/10000], Tolerance: 9.186E-07, Avg. computing time: 0.547s          \n",
      "Parser: TopNGradients, param_ratio: 5%\n",
      "Computing partial influence ... [1/10000], Tolerance: 3.840E-03, Avg. computing time: 0.532s          \r"
     ]
    }
   ],
   "source": [
    "print(\"==> Computing influence..\")\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in range(5,31,5):\n",
    "        print(f\"Parser: {net_parser.__class__.__name__}, param_ratio: {param_ratio}%\")\n",
    "        param_ratio *= 0.01\n",
    "        \n",
    "        # Initialize configurations\n",
    "        net = load_net(net, net_path)\n",
    "        net_parser.num_choices = int(num_param * param_ratio)\n",
    "\n",
    "        # Prepare losses and indexes\n",
    "        total_loss = forward(net, train_loader, criterion, 1)\n",
    "        \n",
    "        # Register_hooks\n",
    "        net_parser.initialize_neurons()\n",
    "        net_parser.register_hooks()\n",
    "        target_loss = (\n",
    "            criterion(net(sample_data.to(device)), sample_target.to(device))\n",
    "            * len(data)\n",
    "            / len(train_loader.dataset)\n",
    "        )\n",
    "        if isinstance(net_parser, selection.TopNGradients):\n",
    "            target_loss.backward(retain_graph=True)\n",
    "\n",
    "        data_ratio = len(train_loader.dataset) / (len(train_loader.dataset) - len(data))\n",
    "        newton_loss = total_loss * data_ratio - target_loss * (1 - data_ratio)\n",
    "        index_list = net_parser.get_parameters()\n",
    "\n",
    "        influence = hessians.partial_influence(\n",
    "            index_list, target_loss, newton_loss, net, tol=1e-6, step=3\n",
    "        )\n",
    "        utils.update_network(net, influence, index_list)\n",
    "        net_parser.remove_hooks()\n",
    "        net_path = (\n",
    "            f\"checkpoints/Figure_3/PIF/{net_name}/{net_parser.__class__.__name__}.pth\"\n",
    "        )\n",
    "        save_net(net, net_path)\n",
    "\n",
    "        net_parser.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92865",
   "metadata": {},
   "source": [
    "### Measure the network utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, dataloader, criterion, label, include):\n",
    "    net_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        if include:\n",
    "            idx = (targets == label)\n",
    "        else:\n",
    "            idx = (targets != label)\n",
    "        inputs = inputs[idx]\n",
    "        targets = targets[idx]\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        net_loss += loss\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    net_loss /= len(dataloader)\n",
    "    return net_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in range(10,11,30):\n",
    "        _, _, test_loader = data_loader.get_data_loaders()\n",
    "        param_ratio *= 0.01\n",
    "        net_path = f\"../checkpoints/Figure_3/class_removal/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        net = load_net(net, net_path)\n",
    "        \n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "        print(f\"{net_parser.__class__.__name__}, {param_ratio*100:2.0f}% - Self: {self_loss:.4f} {self_acc:.2f}% | exclusive loss: {exclusive_loss:.4f}, {exclusive_acc:.2f}%\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "net_path = \"../checkpoints/Figure_3/FullyConnectedNet/cross_entropy/ckpt_0.0.pth\"\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "loss, acc = test(net, test_loader, criterion, 11, False)\n",
    "print(f\"Total loss: {loss:.4f}, {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262802f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
