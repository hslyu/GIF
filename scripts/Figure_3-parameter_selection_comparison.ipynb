{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import FullyConnectedNet, TinyNet, ResNet18\n",
    "from src import hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e7aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(net, dataloader, criterion, num_batch_sample: int=-1):\n",
    "    net_loss = 0\n",
    "    num_batch_sample = len(dataloader) if num_batch_sample == -1 else num_batch_sample\n",
    "    sample_indices = np.random.choice(len(dataloader), size=num_batch_sample, replace=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if batch_idx in sample_indices:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss\n",
    "\n",
    "    net_loss /= num_batch_sample\n",
    "    return net_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f993d",
   "metadata": {},
   "source": [
    "### Building model and set criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96121842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet finished. \n",
      "    Number of parameters: 11172810\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "net = ResNet18(1).to(device)\n",
    "flatten = False\n",
    "net_path = \"../checkpoints/Figure_3/ResNet/cross_entropy/ckpt_0.0.pth\"\n",
    "\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "net_name = net.__class__.__name__\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5ef1c",
   "metadata": {},
   "source": [
    "## Preparing data and register hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24962f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 256\n",
    "num_workers = 12\n",
    "num_batch_sample = 3\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, flatten=flatten)\n",
    "train_loader, val_loader, test_loader = data_loader.get_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15956a",
   "metadata": {},
   "source": [
    "### Prepare sample total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd28ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_list = list()\n",
    "targets_list = list()\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    if batch_idx < num_batch_sample:\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd29223",
   "metadata": {},
   "source": [
    "### Prepare removal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b393bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "target = list()\n",
    "for batch_idx, (data_raw, target_raw) in enumerate(train_loader):\n",
    "    idx = target_raw == 8\n",
    "    data_raw = data_raw[idx]\n",
    "    target_raw = target_raw[idx]\n",
    "    data.append(data_raw)\n",
    "    target.append(target_raw)\n",
    "data = torch.cat(data)\n",
    "target = torch.cat(target)\n",
    "\n",
    "sample_idx = np.random.choice(len(data), 50, replace=False)\n",
    "sample_data = data[sample_idx]\n",
    "sample_target = target[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b2bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Define hooks\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Define hooks\")\n",
    "# Make hooks\n",
    "parser_list = [selection.TopNActivations, \n",
    "                  selection.TopNGradients,\n",
    "                  selection.RandomSelection,\n",
    "                  selection.Threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95cb0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing influence..\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Computing influence..\")\n",
    "\n",
    "ratio_list = [1, 3, 5, 10, 15, 20, 25, 30, 50, 60, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0928c281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser: TopNActivations, param_ratio: 1%\n",
      "0.00435, 0.00033\n",
      "Computing partial influence ... [1/10000], Tolerance: 7.957E-05, Avg. computing time: 0.799s          \n",
      "Parser: TopNActivations, param_ratio: 3%\n",
      "0.00435, 0.00033\n",
      "Computing partial influence ... [1/10000], Tolerance: 8.655E-05, Avg. computing time: 0.816s          \n",
      "Parser: TopNActivations, param_ratio: 5%\n",
      "0.00435, 0.00033\n",
      "Computing partial influence ... [1/10000], Tolerance: 1.290E-04, Avg. computing time: 0.824s          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m newton_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m*\u001b[39m data_ratio \u001b[38;5;241m-\u001b[39m target_loss \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m data_ratio)\n\u001b[1;32m     35\u001b[0m index_list \u001b[38;5;241m=\u001b[39m net_parser\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[0;32m---> 37\u001b[0m influence \u001b[38;5;241m=\u001b[39m \u001b[43mhessians\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_influence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewton_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m utils\u001b[38;5;241m.\u001b[39mupdate_network(net, influence, index_list)\n\u001b[1;32m     41\u001b[0m net_parser\u001b[38;5;241m.\u001b[39mremove_hooks()\n",
      "File \u001b[0;32m~/research/PIF/src/hessians.py:162\u001b[0m, in \u001b[0;36mpartial_influence\u001b[0;34m(index_list, target_loss, total_loss, model, tol, step, verbose, normalizer)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     v \u001b[38;5;241m=\u001b[39m hvp(\n\u001b[1;32m    158\u001b[0m         total_loss \u001b[38;5;241m/\u001b[39m normalizer,\n\u001b[1;32m    159\u001b[0m         model,\n\u001b[1;32m    160\u001b[0m         compute_gradient(target_loss \u001b[38;5;241m/\u001b[39m normalizer, model),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0m     PIF \u001b[38;5;241m=\u001b[39m \u001b[43miphvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PIF \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/research/PIF/src/hessians.py:218\u001b[0m, in \u001b[0;36miphvp\u001b[0;34m(index_list, loss, model, v, tol)\u001b[0m\n\u001b[1;32m    216\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    217\u001b[0m IHVP_old \u001b[38;5;241m=\u001b[39m IHVP_new\n\u001b[0;32m--> 218\u001b[0m IHVP_new \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m+\u001b[39m IHVP_old \u001b[38;5;241m-\u001b[39m \u001b[43msHVP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIHVP_old\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(IHVP_new \u001b[38;5;241m-\u001b[39m IHVP_old)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/research/PIF/src/hessians.py:203\u001b[0m, in \u001b[0;36miphvp.<locals>.sHVP\u001b[0;34m(index_list, loss, model, v)\u001b[0m\n\u001b[1;32m    201\u001b[0m zeropad_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(num_params, device\u001b[38;5;241m=\u001b[39mv\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    202\u001b[0m zeropad_v[index_list] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m--> 203\u001b[0m twice_HVP \u001b[38;5;241m=\u001b[39m \u001b[43mhvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzeropad_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m twice_HVP[index_list]\n",
      "File \u001b[0;32m~/research/PIF/src/hessians.py:101\u001b[0m, in \u001b[0;36mhvp\u001b[0;34m(loss, model, v, create_graph)\u001b[0m\n\u001b[1;32m     97\u001b[0m grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     98\u001b[0m     loss, \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()), create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([grad\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads])\n\u001b[0;32m--> 101\u001b[0m hvp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([grad\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m hvp])\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        print(f\"Parser: {net_parser.__class__.__name__}, param_ratio: {param_ratio}%\")\n",
    "        param_ratio *= 0.01\n",
    "        \n",
    "        # Initialize configurations\n",
    "        net = load_net(net, net_path)\n",
    "        net_parser.set_ratio(param_ratio)\n",
    "\n",
    "        # Prepare losses and indexes\n",
    "        total_loss = 0\n",
    "        for inputs, targets in zip(inputs_list, targets_list):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_batch_sample\n",
    "\n",
    "        # Register_hooks\n",
    "        net_parser.initialize_neurons()\n",
    "        net_parser.register_hooks()\n",
    "        target_loss = (\n",
    "            criterion(net(sample_data.to(device)), sample_target.to(device))\n",
    "            * len(data)\n",
    "            / len(train_loader.dataset)\n",
    "        )\n",
    "        if isinstance(net_parser, selection.TopNGradients):\n",
    "            target_loss.backward(retain_graph=True)\n",
    "\n",
    "        data_ratio = len(train_loader.dataset) / (len(train_loader.dataset) - len(data))\n",
    "        newton_loss = total_loss * data_ratio - target_loss * (1 - data_ratio)\n",
    "        index_list = net_parser.get_parameters()\n",
    "\n",
    "        influence = hessians.partial_influence(\n",
    "            index_list, target_loss, newton_loss, net, tol=1e-3, step=.5\n",
    "        )\n",
    "        utils.update_network(net, influence, index_list)\n",
    "        net_parser.remove_hooks()\n",
    "        save_path = (\n",
    "            f\"../checkpoints/Figure_3/PIF/{net_name}/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        )\n",
    "        save_net(net, save_path)\n",
    "\n",
    "        net_parser.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92865",
   "metadata": {},
   "source": [
    "### Measure the network utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, dataloader, criterion, label, include):\n",
    "    net_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        if include:\n",
    "            idx = (targets == label)\n",
    "        else:\n",
    "            idx = (targets != label)\n",
    "        inputs = inputs[idx]\n",
    "        targets = targets[idx]\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        net_loss += loss\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    net_loss /= len(dataloader)\n",
    "    return net_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "\n",
    "# Define lists to contain results\n",
    "self_loss_list = [[],[],[],[]]\n",
    "self_acc_list = [[],[],[],[]]\n",
    "exclusive_loss_list = [[],[],[],[]]\n",
    "exclusive_acc_list = [[],[],[],[]]\n",
    "\n",
    "parser_count = 0\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        _, _, test_loader = data_loader.get_data_loaders()\n",
    "        param_ratio *= 0.01\n",
    "        #net_path = f\"../checkpoints/Figure_3/class_removal/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        net_path = f\"../checkpoints/Figure_3/PIF/ResNet/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        net = load_net(net, net_path)\n",
    "        \n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        self_loss_list[parser_count].append(self_loss.detach().cpu())\n",
    "        self_acc_list[parser_count].append(self_acc)\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "        \n",
    "        # Save results in defined lists\n",
    "        exclusive_loss_list[parser_count].append(exclusive_loss.detach().cpu())\n",
    "        exclusive_acc_list[parser_count].append(exclusive_acc)\n",
    "        \n",
    "        print(f\"{net_parser.__class__.__name__}, {param_ratio*100:2.0f}% - Self: {self_loss:.4f} {self_acc:.2f}% | exclusive loss: {exclusive_loss:.4f}, {exclusive_acc:.2f}%\")\n",
    "    print(\"\")\n",
    "    parser_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Show results\n",
    "print(\"Self Loss\")\n",
    "data = {\"A\": self_loss_list[0],\n",
    "        \"B\": self_loss_list[1],\n",
    "        \"C\": self_loss_list[2],\n",
    "        \"D\": self_loss_list[3],\n",
    "       }\n",
    "self_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_loss_df = self_loss_df.transpose()\n",
    "self_loss_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(self_loss_df)\n",
    "\n",
    "print(\"Self Accuracy\")\n",
    "data = {\"A\": self_acc_list[0],\n",
    "        \"B\": self_acc_list[1],\n",
    "        \"C\": self_acc_list[2],\n",
    "        \"D\": self_acc_list[3],\n",
    "       }\n",
    "self_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_acc_df = self_acc_df.transpose()\n",
    "self_acc_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(self_acc_df)\n",
    "\n",
    "print(\"Exclusive Loss\")\n",
    "data = {\"A\": exclusive_loss_list[0],\n",
    "        \"B\": exclusive_loss_list[1],\n",
    "        \"C\": exclusive_loss_list[2],\n",
    "        \"D\": exclusive_loss_list[3],\n",
    "       }\n",
    "exclusive_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_loss_df = exclusive_loss_df.transpose()\n",
    "exclusive_loss_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(exclusive_loss_df)\n",
    "\n",
    "print(\"Exclusive Accuracy\")\n",
    "data = {\"A\": exclusive_acc_list[0],\n",
    "        \"B\": exclusive_acc_list[1],\n",
    "        \"C\": exclusive_acc_list[2],\n",
    "        \"D\": exclusive_acc_list[3],\n",
    "       }\n",
    "exclusive_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_acc_df = exclusive_acc_df.transpose()\n",
    "exclusive_acc_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(exclusive_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save list files of results\n",
    "with open('self_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_acc, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('exclusive_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_acc, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262802f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
