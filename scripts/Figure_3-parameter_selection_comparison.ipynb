{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import FullyConnectedNet, TinyNet, ResNet18\n",
    "from src import hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e7aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(net, dataloader, criterion, num_batch_sample: int=-1):\n",
    "    net_loss = 0\n",
    "    num_batch_sample = len(dataloader) if num_batch_sample == -1 else num_batch_sample\n",
    "    sample_indices = np.random.choice(len(dataloader), size=num_batch_sample, replace=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if batch_idx in sample_indices:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss\n",
    "\n",
    "    net_loss /= num_batch_sample\n",
    "    return net_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f993d",
   "metadata": {},
   "source": [
    "### Building model and set criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96121842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet finished. \n",
      "    Number of parameters: 11172810\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "net = ResNet18(1).to(device)\n",
    "flatten = False\n",
    "net_path = \"../checkpoints/Figure_3/ResNet/cross_entropy/ckpt_0.0.pth\"\n",
    "\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "net_name = net.__class__.__name__\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10c6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, dataloader, criterion, label, include):\n",
    "    with torch.no_grad():\n",
    "        net_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            if include:\n",
    "                idx = (targets == label)\n",
    "            else:\n",
    "                idx = (targets != label)\n",
    "            inputs = inputs[idx]\n",
    "            targets = targets[idx]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss\n",
    "\n",
    "            total += targets.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total * 100\n",
    "        net_loss /= len(dataloader)\n",
    "        return net_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5ef1c",
   "metadata": {},
   "source": [
    "## Preparing data and register hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24962f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 12\n",
    "num_batch_sample = 3\n",
    "num_target_sample = 300\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, flatten=flatten)\n",
    "train_loader, val_loader, test_loader = data_loader.get_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15956a",
   "metadata": {},
   "source": [
    "### Prepare sample total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd28ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_list = list()\n",
    "targets_list = list()\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    if batch_idx < num_batch_sample:\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd29223",
   "metadata": {},
   "source": [
    "### Prepare removal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b393bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "target = list()\n",
    "for batch_idx, (data_raw, target_raw) in enumerate(train_loader):\n",
    "    idx = target_raw == 8\n",
    "    data_raw = data_raw[idx]\n",
    "    target_raw = target_raw[idx]\n",
    "    data.append(data_raw)\n",
    "    target.append(target_raw)\n",
    "data = torch.cat(data)\n",
    "target = torch.cat(target)\n",
    "\n",
    "sample_idx = np.random.choice(len(data), num_target_sample, replace=False)\n",
    "sample_data = data[sample_idx]\n",
    "sample_target = target[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63b2bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Define hooks\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Define hooks\")\n",
    "# Make hooks\n",
    "parser_list = [selection.TopNActivations, \n",
    "                  selection.TopNGradients,\n",
    "                  selection.RandomSelection,\n",
    "                  selection.Threshold]\n",
    "\n",
    "rest_parser_list = [selection.TopNActivations, \n",
    "                  selection.TopNGradients,\n",
    "                  selection.RandomSelection,\n",
    "                  selection.Threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95cb0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing influence..\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Computing influence..\")\n",
    "\n",
    "ratio_list = [1,   5,  10,  30,  50, 100]\n",
    "scale_list = [190, 150, 95,  85,  28,   5]\n",
    "#scale_list = [200, 150, 100,  85,  28,   5]\n",
    "\n",
    "#ratio_list = [10]\n",
    "#scale_list = [95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0928c281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser: TopNActivations, param_ratio: 10%\n",
      "Computing partial influence ... [1/10000], Tolerance: 4.559E-05, Avg. computing time: 1.952s          \n",
      "Parser: TopNGradients, param_ratio: 10%\n",
      "Computing partial influence ... [1/10000], Tolerance: 4.638E-05, Avg. computing time: 1.951s          \n",
      "Parser: RandomSelection, param_ratio: 10%\n",
      "Computing partial influence ... [1/10000], Tolerance: 3.474E-05, Avg. computing time: 1.953s          \n",
      "Parser: Threshold, param_ratio: 10%\n",
      "Computing partial influence ... [1/10000], Tolerance: 3.898E-05, Avg. computing time: 1.952s          \n"
     ]
    }
   ],
   "source": [
    "for parser in rest_parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio, inf_scale in zip(ratio_list, scale_list):\n",
    "        print(f\"Parser: {net_parser.__class__.__name__}, param_ratio: {param_ratio}%\")\n",
    "        param_ratio *= 0.01\n",
    "        \n",
    "        # Initialize configurations\n",
    "        net_path = \"../checkpoints/Figure_3/ResNet/cross_entropy/ckpt_0.0.pth\"\n",
    "        net = load_net(net, net_path)\n",
    "        net_parser.set_ratio(param_ratio)\n",
    "\n",
    "        # Prepare losses and indexes\n",
    "        total_loss = 0\n",
    "        for inputs, targets in zip(inputs_list, targets_list):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_batch_sample\n",
    "\n",
    "        # Register_hooks\n",
    "        net_parser.initialize_neurons()\n",
    "        net_parser.register_hooks()\n",
    "        target_loss = (\n",
    "            criterion(net(sample_data.to(device)), sample_target.to(device))\n",
    "            * len(data)\n",
    "            / len(train_loader.dataset)\n",
    "        )\n",
    "        if isinstance(net_parser, selection.TopNGradients):\n",
    "            target_loss.backward(retain_graph=True)\n",
    "\n",
    "        data_ratio = len(train_loader.dataset) / (len(train_loader.dataset) - len(data))\n",
    "        #newton_loss = total_loss * data_ratio - target_loss * (1 - data_ratio)\n",
    "        index_list = net_parser.get_parameters()\n",
    "\n",
    "        influence = hessians.partial_influence(\n",
    "            index_list, target_loss, total_loss, net, tol=1e-4, step=0.5 #3\n",
    "        )\n",
    "        \n",
    "        net = load_net(net, net_path)\n",
    "        net_parser.set_ratio(param_ratio)\n",
    "\n",
    "        utils.update_network(net, influence*inf_scale, index_list)\n",
    "        net_parser.remove_hooks()\n",
    "        save_path = (\n",
    "            #f\"../checkpoints/Figure_3/PIF/{net_name}/{net_parser.__class__.__name__}/{param_ratio}/{inf_scale}.pth\"\n",
    "            f\"../checkpoints/Figure_3/PIF/{net_name}/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        )\n",
    "        save_net(net, save_path)\n",
    "\n",
    "        net_parser.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92865",
   "metadata": {},
   "source": [
    "### Measure the network utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c76599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopNActivations,  1% - Self: 5.6520 0.10% | exclusive loss: 0.0330, 99.06%\n",
      "\n",
      "TopNActivations,  5% - Self: 6.2325 0.10% | exclusive loss: 0.0471, 98.65%\n",
      "\n",
      "TopNActivations, 10% - Self: 6.2056 0.31% | exclusive loss: 0.1124, 96.74%\n",
      "\n",
      "TopNActivations, 30% - Self: 5.1284 1.64% | exclusive loss: 0.6400, 82.84%\n",
      "\n",
      "TopNActivations, 50% - Self: 4.8905 8.52% | exclusive loss: 0.6932, 84.67%\n",
      "\n",
      "TopNActivations, 100% - Self: 4.7985 17.35% | exclusive loss: 0.5885, 87.92%\n",
      "\n",
      "TopNGradients,  1% - Self: 5.5547 0.82% | exclusive loss: 0.0341, 99.04%\n",
      "\n",
      "TopNGradients,  5% - Self: 6.2369 0.21% | exclusive loss: 0.0432, 98.75%\n",
      "\n",
      "TopNGradients, 10% - Self: 6.2856 0.21% | exclusive loss: 0.1171, 96.67%\n",
      "\n",
      "TopNGradients, 30% - Self: 4.9455 0.92% | exclusive loss: 1.0185, 72.45%\n",
      "\n",
      "TopNGradients, 50% - Self: 4.8896 5.75% | exclusive loss: 0.8093, 79.80%\n",
      "\n",
      "TopNGradients, 100% - Self: 4.7191 16.63% | exclusive loss: 0.5831, 87.89%\n",
      "\n",
      "RandomSelection,  1% - Self: 4.7260 19.82% | exclusive loss: 0.0641, 98.34%\n",
      "\n",
      "RandomSelection,  5% - Self: 4.4170 20.02% | exclusive loss: 0.0831, 98.01%\n",
      "\n",
      "RandomSelection, 10% - Self: 4.7941 5.95% | exclusive loss: 0.5984, 84.09%\n",
      "\n",
      "RandomSelection, 30% - Self: 3.3585 17.15% | exclusive loss: 2.6318, 35.85%\n",
      "\n",
      "RandomSelection, 50% - Self: 4.2592 19.71% | exclusive loss: 1.4630, 66.80%\n",
      "\n",
      "RandomSelection, 100% - Self: 4.3203 18.58% | exclusive loss: 0.8398, 83.38%\n",
      "\n",
      "Threshold,  1% - Self: 4.6772 18.28% | exclusive loss: 0.0550, 98.59%\n",
      "\n",
      "Threshold,  5% - Self: 4.4210 19.61% | exclusive loss: 0.0874, 97.74%\n",
      "\n",
      "Threshold, 10% - Self: 4.8108 4.72% | exclusive loss: 0.6482, 83.28%\n",
      "\n",
      "Threshold, 30% - Self: 3.7192 18.79% | exclusive loss: 1.7797, 58.39%\n",
      "\n",
      "Threshold, 50% - Self: 4.3311 18.99% | exclusive loss: 1.1803, 73.47%\n",
      "\n",
      "Threshold, 100% - Self: 4.2788 18.58% | exclusive loss: 0.8162, 83.64%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "\n",
    "# Define lists to contain results\n",
    "self_loss_list = [[],[],[],[]]\n",
    "self_acc_list = [[],[],[],[]]\n",
    "exclusive_loss_list = [[],[],[],[]]\n",
    "exclusive_acc_list = [[],[],[],[]]\n",
    "\n",
    "parser_count = 0\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        _, _, test_loader = data_loader.get_data_loaders()\n",
    "        param_ratio *= 0.01\n",
    "        \n",
    "        net_path = f\"../checkpoints/Figure_3/PIF/ResNet/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        net = load_net(net, net_path)\n",
    "\n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        self_loss_list[parser_count].append(self_loss.detach().cpu())\n",
    "        self_acc_list[parser_count].append(self_acc)\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "\n",
    "        # Save results in defined lists\n",
    "        exclusive_loss_list[parser_count].append(exclusive_loss.detach().cpu())\n",
    "        exclusive_acc_list[parser_count].append(exclusive_acc)\n",
    "\n",
    "        print(f\"{net_parser.__class__.__name__}, {param_ratio*100:2.0f}% - Self: {self_loss:.4f} {self_acc:.2f}% | exclusive loss: {exclusive_loss:.4f}, {exclusive_acc:.2f}%\")\n",
    "        print(\"\")\n",
    "    parser_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a494b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self Loss\n",
      "                   1%    5%   10%   30%   50%  100%\n",
      "TopNActivations  5.65  6.23  6.21  5.13  4.89  4.80\n",
      "TopNGradients    5.55  6.24  6.29  4.95  4.89  4.72\n",
      "Threshold        4.73  4.42  4.79  3.36  4.26  4.32\n",
      "Random           4.68  4.42  4.81  3.72  4.33  4.28\n",
      "\n",
      "Self Accuracy\n",
      "                    1%     5%   10%    30%    50%   100%\n",
      "TopNActivations   0.10   0.10  0.31   1.64   8.52  17.35\n",
      "TopNGradients     0.82   0.21  0.21   0.92   5.75  16.63\n",
      "Threshold        19.82  20.02  5.95  17.15  19.71  18.58\n",
      "Random           18.28  19.61  4.72  18.79  18.99  18.58\n",
      "\n",
      "Exclusive Loss\n",
      "                   1%    5%   10%   30%   50%  100%\n",
      "TopNActivations  0.03  0.05  0.11  0.64  0.69  0.59\n",
      "TopNGradients    0.03  0.04  0.12  1.02  0.81  0.58\n",
      "Threshold        0.06  0.08  0.60  2.63  1.46  0.84\n",
      "Random           0.06  0.09  0.65  1.78  1.18  0.82\n",
      "\n",
      "Exclusive Accuracy\n",
      "                    1%     5%    10%    30%    50%   100%\n",
      "TopNActivations  99.06  98.65  96.74  82.84  84.67  87.92\n",
      "TopNGradients    99.04  98.75  96.67  72.45  79.80  87.89\n",
      "Threshold        98.34  98.01  84.09  35.85  66.80  83.38\n",
      "Random           98.59  97.74  83.28  58.39  73.47  83.64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Show results\n",
    "print(\"Self Loss\")\n",
    "for i in range(4):\n",
    "    self_loss_list[i] = [float(tensor) for tensor in self_loss_list[i]]\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in self_loss_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in self_loss_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in self_loss_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in self_loss_list[3]],\n",
    "       }\n",
    "self_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_loss_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "self_loss_df = self_loss_df.transpose()\n",
    "print(self_loss_df)\n",
    "\n",
    "print(\"\\nSelf Accuracy\")\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in self_acc_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in self_acc_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in self_acc_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in self_acc_list[3]]\n",
    "       }\n",
    "self_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_acc_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "self_acc_df = self_acc_df.transpose()\n",
    "print(self_acc_df)\n",
    "\n",
    "for i in range(4):\n",
    "    exclusive_loss_list[i] = [float(tensor) for tensor in exclusive_loss_list[i]]\n",
    "print(\"\\nExclusive Loss\")\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in exclusive_loss_list[3]],\n",
    "       }\n",
    "exclusive_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_loss_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "exclusive_loss_df = exclusive_loss_df.transpose()\n",
    "print(exclusive_loss_df)\n",
    "\n",
    "print(\"\\nExclusive Accuracy\")\n",
    "data = {\"A\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[0]],\n",
    "        \"B\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[1]],\n",
    "        \"C\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[2]],\n",
    "        \"D\": [\"{:.2f}\".format(num) for num in exclusive_acc_list[3]],\n",
    "       }\n",
    "exclusive_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_acc_df.columns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "exclusive_acc_df = exclusive_acc_df.transpose()\n",
    "print(exclusive_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "471cd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save list files of results\n",
    "with open('self_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_acc, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('exclusive_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_acc, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262802f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
