{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import FullyConnectedNet, TinyNet, ResNet18\n",
    "from src import hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e7aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(net, dataloader, criterion, num_batch_sample: int=-1):\n",
    "    net_loss = 0\n",
    "    num_batch_sample = len(dataloader) if num_batch_sample == -1 else num_batch_sample\n",
    "    sample_indices = np.random.choice(len(dataloader), size=num_batch_sample, replace=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if batch_idx in sample_indices:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss\n",
    "\n",
    "    net_loss /= num_batch_sample\n",
    "    return net_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f993d",
   "metadata": {},
   "source": [
    "### Building model and set criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96121842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet finished. \n",
      "    Number of parameters: 11172810\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "net = ResNet18(1).to(device)\n",
    "flatten = False\n",
    "net_path = \"../checkpoints/Figure_3/ResNet/cross_entropy/ckpt_0.0.pth\"\n",
    "\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "net_name = net.__class__.__name__\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5ef1c",
   "metadata": {},
   "source": [
    "### Preparing data and register hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b393bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, flatten=flatten)\n",
    "train_loader, val_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "data = list()\n",
    "target = list()\n",
    "for batch_idx, (data_raw, target_raw) in enumerate(train_loader):\n",
    "    idx = target_raw == 8\n",
    "    data_raw = data_raw[idx]\n",
    "    target_raw = target_raw[idx]\n",
    "    data.append(data_raw)\n",
    "    target.append(target_raw)\n",
    "data = torch.cat(data)\n",
    "target = torch.cat(target)\n",
    "sample_idx = np.random.choice(len(data), 50, replace=False)\n",
    "sample_data = data[sample_idx]\n",
    "sample_target = target[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b2bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Define hooks\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Define hooks\")\n",
    "# Make hooks\n",
    "parser_list = [selection.TopNActivations, \n",
    "                  selection.TopNGradients,\n",
    "                  selection.RandomSelection,\n",
    "                  selection.Threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928c281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing influence..\n",
      "Parser: TopNActivations, param_ratio: 1%\n",
      "Computing partial influence ... [113/10000], Tolerance: 9.969E-08, Avg. computing time: 0.524s          \n",
      "Parser: TopNActivations, param_ratio: 3%\n",
      "Computing partial influence ... [145/10000], Tolerance: 9.970E-08, Avg. computing time: 0.527s          \n",
      "Parser: TopNActivations, param_ratio: 5%\n",
      "Computing partial influence ... [13/10000], Tolerance: 9.572E-08, Avg. computing time: 0.528s          \n",
      "Parser: TopNActivations, param_ratio: 10%\n",
      "Computing partial influence ... [67/10000], Tolerance: 9.947E-08, Avg. computing time: 0.529s          \n",
      "Parser: TopNActivations, param_ratio: 15%\n",
      "Computing partial influence ... [17/10000], Tolerance: 9.847E-08, Avg. computing time: 0.530s          \n",
      "Parser: TopNActivations, param_ratio: 20%\n",
      "Computing partial influence ... [94/10000], Tolerance: 9.931E-08, Avg. computing time: 0.531s          \n",
      "Parser: TopNActivations, param_ratio: 25%\n",
      "Computing partial influence ... [664/10000], Tolerance: 9.996E-08, Avg. computing time: 0.536s          \n",
      "Parser: TopNActivations, param_ratio: 30%\n",
      "Computing partial influence ... [414/10000], Tolerance: 9.944E-08, Avg. computing time: 0.538s          \n",
      "Parser: TopNActivations, param_ratio: 50%\n",
      "Computing partial influence ... [269/10000], Tolerance: 9.992E-08, Avg. computing time: 0.543s          \n",
      "Parser: TopNActivations, param_ratio: 60%\n",
      "Computing partial influence ... [268/10000], Tolerance: 9.816E-08, Avg. computing time: 0.546s          \n",
      "Parser: TopNActivations, param_ratio: 75%\n",
      "Computing partial influence ... [994/10000], Tolerance: 9.992E-08, Avg. computing time: 0.549s          \n",
      "Parser: TopNActivations, param_ratio: 100%\n",
      "Computing partial influence ... [655/10000], Tolerance: 9.957E-08, Avg. computing time: 0.558s          \n",
      "Parser: TopNGradients, param_ratio: 1%\n",
      "Computing partial influence ... [601/10000], Tolerance: 6.213E-07, Avg. computing time: 0.529s           \r"
     ]
    }
   ],
   "source": [
    "print(\"==> Computing influence..\")\n",
    "\n",
    "ratio_list = [1, 3, 5, 10, 15, 20, 25, 30, 50, 60, 75, 100]\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        print(f\"Parser: {net_parser.__class__.__name__}, param_ratio: {param_ratio}%\")\n",
    "        param_ratio *= 0.01\n",
    "        \n",
    "        # Initialize configurations\n",
    "        net = load_net(net, net_path)\n",
    "        net_parser.set_ratio(param_ratio)\n",
    "\n",
    "        # Prepare losses and indexes\n",
    "        total_loss = forward(net, train_loader, criterion, 1)\n",
    "        \n",
    "        # Register_hooks\n",
    "        net_parser.initialize_neurons()\n",
    "        net_parser.register_hooks()\n",
    "        target_loss = (\n",
    "            criterion(net(sample_data.to(device)), sample_target.to(device))\n",
    "            * len(data)\n",
    "            / len(train_loader.dataset)\n",
    "        )\n",
    "        if isinstance(net_parser, selection.TopNGradients):\n",
    "            target_loss.backward(retain_graph=True)\n",
    "\n",
    "        data_ratio = len(train_loader.dataset) / (len(train_loader.dataset) - len(data))\n",
    "        newton_loss = total_loss * data_ratio - target_loss * (1 - data_ratio)\n",
    "        index_list = net_parser.get_parameters()\n",
    "\n",
    "        influence = hessians.partial_influence(\n",
    "            index_list, target_loss, newton_loss, net, tol=1e-7, step=3\n",
    "        )\n",
    "        utils.update_network(net, influence, index_list)\n",
    "        net_parser.remove_hooks()\n",
    "        net_path = (\n",
    "            f\"../checkpoints/Figure_3/PIF/{net_name}/{net_parser.__class__.__name__}.pth\"\n",
    "        )\n",
    "        save_net(net, net_path)\n",
    "\n",
    "        net_parser.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92865",
   "metadata": {},
   "source": [
    "### Measure the network utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, dataloader, criterion, label, include):\n",
    "    net_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        if include:\n",
    "            idx = (targets == label)\n",
    "        else:\n",
    "            idx = (targets != label)\n",
    "        inputs = inputs[idx]\n",
    "        targets = targets[idx]\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        net_loss += loss\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    net_loss /= len(dataloader)\n",
    "    return net_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "\n",
    "# Define lists to contain results\n",
    "self_loss_list = [[],[],[],[]]\n",
    "self_acc_list = [[],[],[],[]]\n",
    "exclusive_loss_list = [[],[],[],[]]\n",
    "exclusive_acc_list = [[],[],[],[]]\n",
    "\n",
    "parser_count = 0\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        _, _, test_loader = data_loader.get_data_loaders()\n",
    "        param_ratio *= 0.01\n",
    "        net_path = f\"../checkpoints/Figure_3/class_removal/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        net = load_net(net, net_path)\n",
    "        \n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "        \n",
    "        # Save results in defined lists\n",
    "        self_loss_list[parser_count].append(self_loss)\n",
    "        self_acc_list[parser_count].append(self_acc)\n",
    "        exclusive_loss_list[parser_count].append(exclusive_loss)\n",
    "        exclusive_acc_list[parser_count].append(exclusive_acc)\n",
    "        \n",
    "        print(f\"{net_parser.__class__.__name__}, {param_ratio*100:2.0f}% - Self: {self_loss:.4f} {self_acc:.2f}% | exclusive loss: {exclusive_loss:.4f}, {exclusive_acc:.2f}%\")\n",
    "    print(\"\")\n",
    "    parser_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Show results\n",
    "print(\"Self Loss\")\n",
    "data = {\"A\": self_loss_list[0],\n",
    "        \"B\": self_loss_list[1],\n",
    "        \"C\": self_loss_list[2],\n",
    "        \"D\": self_loss_list[3],\n",
    "       }\n",
    "self_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_loss_df = self_loss_df.transpose()\n",
    "self_loss_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(self_loss_df)\n",
    "\n",
    "print(\"Self Accuracy\")\n",
    "data = {\"A\": self_acc_list[0],\n",
    "        \"B\": self_acc_list[1],\n",
    "        \"C\": self_acc_list[2],\n",
    "        \"D\": self_acc_list[3],\n",
    "       }\n",
    "self_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_acc_df = self_acc_df.transpose()\n",
    "self_acc_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(self_acc_df)\n",
    "\n",
    "print(\"Exclusive Loss\")\n",
    "data = {\"A\": exclusive_loss_list[0],\n",
    "        \"B\": exclusive_loss_list[1],\n",
    "        \"C\": exclusive_loss_list[2],\n",
    "        \"D\": exclusive_loss_list[3],\n",
    "       }\n",
    "exclusive_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_loss_df = exclusive_loss_df.transpose()\n",
    "exclusive_loss_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(exclusive_loss_df)\n",
    "\n",
    "print(\"Exclusive Accuracy\")\n",
    "data = {\"A\": exclusive_acc_list[0],\n",
    "        \"B\": exclusive_acc_list[1],\n",
    "        \"C\": exclusive_acc_list[2],\n",
    "        \"D\": exclusive_acc_list[3],\n",
    "       }\n",
    "exclusive_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_acc_df = exclusive_acc_df.transpose()\n",
    "exclusive_acc_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(exclusive_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save list files of results\n",
    "with open('self_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_acc, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('exclusive_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_acc, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "net_path = \"../checkpoints/Figure_3/FullyConnectedNet/cross_entropy/ckpt_0.0.pth\"\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "loss, acc = test(net, test_loader, criterion, 11, False)\n",
    "print(f\"Total loss: {loss:.4f}, {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262802f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
