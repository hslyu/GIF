{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0f03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import FullyConnectedNet, TinyNet, ResNet18\n",
    "from src import hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e7aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(net, dataloader, criterion, num_batch_sample: int=-1):\n",
    "    net_loss = 0\n",
    "    num_batch_sample = len(dataloader) if num_batch_sample == -1 else num_batch_sample\n",
    "    sample_indices = np.random.choice(len(dataloader), size=num_batch_sample, replace=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if batch_idx in sample_indices:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss\n",
    "\n",
    "    net_loss /= num_batch_sample\n",
    "    return net_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f993d",
   "metadata": {},
   "source": [
    "### Building model and set criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96121842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet finished. \n",
      "    Number of parameters: 11172810\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "net = ResNet18(1).to(device)\n",
    "flatten = False\n",
    "net_path = \"../checkpoints/Figure_3/ResNet/cross_entropy/ckpt_0.0.pth\"\n",
    "\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "net_name = net.__class__.__name__\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5ef1c",
   "metadata": {},
   "source": [
    "### Preparing data and register hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b393bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, flatten=flatten)\n",
    "train_loader, val_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "data = list()\n",
    "target = list()\n",
    "for batch_idx, (data_raw, target_raw) in enumerate(train_loader):\n",
    "    idx = target_raw == 8\n",
    "    data_raw = data_raw[idx]\n",
    "    target_raw = target_raw[idx]\n",
    "    data.append(data_raw)\n",
    "    target.append(target_raw)\n",
    "data = torch.cat(data)\n",
    "target = torch.cat(target)\n",
    "sample_idx = np.random.choice(len(data), 50, replace=False)\n",
    "sample_data = data[sample_idx]\n",
    "sample_target = target[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b2bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Define hooks\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Define hooks\")\n",
    "# Make hooks\n",
    "parser_list = [selection.TopNActivations, \n",
    "                  selection.TopNGradients,\n",
    "                  selection.RandomSelection,\n",
    "                  selection.Threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95cb0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing influence..\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Computing influence..\")\n",
    "\n",
    "ratio_list = [1, 3, 5, 10, 15, 20, 25, 30, 50, 60, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0928c281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing influence..\n",
      "Parser: Threshold, param_ratio: 1%\n",
      "Computing partial influence ... [370/10000], Tolerance: 9.918E-08, Avg. computing time: 0.528s          \n",
      "Parser: Threshold, param_ratio: 3%\n",
      "Computing partial influence ... [35/10000], Tolerance: 9.805E-08, Avg. computing time: 0.528s          \n",
      "Parser: Threshold, param_ratio: 5%\n",
      "Computing partial influence ... [22/10000], Tolerance: 9.904E-08, Avg. computing time: 0.527s          \n",
      "Parser: Threshold, param_ratio: 10%\n",
      "Computing partial influence ... [15/10000], Tolerance: 9.883E-08, Avg. computing time: 0.528s          \n",
      "Parser: Threshold, param_ratio: 15%\n",
      "Computing partial influence ... [13/10000], Tolerance: 9.927E-08, Avg. computing time: 0.529s          \n",
      "Parser: Threshold, param_ratio: 20%\n",
      "Computing partial influence ... [4/10000], Tolerance: 8.001E-08, Avg. computing time: 0.530s          \n",
      "Parser: Threshold, param_ratio: 25%\n",
      "Computing partial influence ... [2/10000], Tolerance: 8.172E-08, Avg. computing time: 0.531s          \n",
      "Parser: Threshold, param_ratio: 30%\n",
      "Computing partial influence ... [3/10000], Tolerance: 9.957E-08, Avg. computing time: 0.532s          \n",
      "Parser: Threshold, param_ratio: 50%\n",
      "Computing partial influence ... [2/10000], Tolerance: 9.901E-08, Avg. computing time: 0.539s          \n",
      "Parser: Threshold, param_ratio: 60%\n",
      "Computing partial influence ... [3/10000], Tolerance: 8.249E-08, Avg. computing time: 0.541s          \n",
      "Parser: Threshold, param_ratio: 75%\n",
      "Computing partial influence ... [1/10000], Tolerance: 8.600E-08, Avg. computing time: 0.546s          \n",
      "Parser: Threshold, param_ratio: 100%\n",
      "Computing partial influence ... [3/10000], Tolerance: 8.296E-08, Avg. computing time: 0.552s          \n"
     ]
    }
   ],
   "source": [
    "rest_parser_list = [#selection.TopNActivations, \n",
    "                  #selection.TopNGradients,\n",
    "                  #selection.RandomSelection,\n",
    "                  selection.Threshold]\n",
    "\n",
    "for parser in rest_parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        print(f\"Parser: {net_parser.__class__.__name__}, param_ratio: {param_ratio}%\")\n",
    "        param_ratio *= 0.01\n",
    "        \n",
    "        # Initialize configurations\n",
    "        net = load_net(net, net_path)\n",
    "        net_parser.set_ratio(param_ratio)\n",
    "\n",
    "        # Prepare losses and indexes\n",
    "        total_loss = forward(net, train_loader, criterion, 1)\n",
    "        \n",
    "        # Register_hooks\n",
    "        net_parser.initialize_neurons()\n",
    "        net_parser.register_hooks()\n",
    "        target_loss = (\n",
    "            criterion(net(sample_data.to(device)), sample_target.to(device))\n",
    "            * len(data)\n",
    "            / len(train_loader.dataset)\n",
    "        )\n",
    "        if isinstance(net_parser, selection.TopNGradients):\n",
    "            target_loss.backward(retain_graph=True)\n",
    "\n",
    "        data_ratio = len(train_loader.dataset) / (len(train_loader.dataset) - len(data))\n",
    "        newton_loss = total_loss * data_ratio - target_loss * (1 - data_ratio)\n",
    "        index_list = net_parser.get_parameters()\n",
    "\n",
    "        influence = hessians.partial_influence(\n",
    "            index_list, target_loss, newton_loss, net, tol=1e-7, step=3\n",
    "        )\n",
    "        utils.update_network(net, influence, index_list)\n",
    "        net_parser.remove_hooks()\n",
    "        net_path = (\n",
    "            f\"../checkpoints/Figure_3/PIF/{net_name}/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        )\n",
    "        save_net(net, net_path)\n",
    "\n",
    "        net_parser.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92865",
   "metadata": {},
   "source": [
    "### Measure the network utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10c6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, dataloader, criterion, label, include):\n",
    "    net_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for _, (inputs, targets) in enumerate(dataloader):\n",
    "        if include:\n",
    "            idx = (targets == label)\n",
    "        else:\n",
    "            idx = (targets != label)\n",
    "        inputs = inputs[idx]\n",
    "        targets = targets[idx]\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        net_loss += loss\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    net_loss /= len(dataloader)\n",
    "    return net_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c76599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopNActivations,  1% - Self: 4.4269 20.53%\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.70 GiB total capacity; 21.17 GiB already allocated; 81.69 MiB free; 22.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet_parser\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_ratio\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% - Self: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mself_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mself_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m self_loss, self_acc\n\u001b[0;32m---> 25\u001b[0m exclusive_loss, exclusive_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Save results in defined lists\u001b[39;00m\n\u001b[1;32m     28\u001b[0m exclusive_loss_list[parser_count]\u001b[38;5;241m.\u001b[39mappend(exclusive_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(net, dataloader, criterion, label, include)\u001b[0m\n\u001b[1;32m     11\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets[idx]\n\u001b[1;32m     12\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     15\u001b[0m net_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PIF/models/resnet.py:112\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    111\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m--> 112\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n\u001b[1;32m    114\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(out)\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PIF/models/resnet.py:42\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 42\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     43\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out))\n\u001b[1;32m     44\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(x)\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.70 GiB total capacity; 21.17 GiB already allocated; 81.69 MiB free; 22.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "net = ResNet18(1).to(device)\n",
    "\n",
    "# Define lists to contain results\n",
    "self_loss_list = [[],[],[],[]]\n",
    "self_acc_list = [[],[],[],[]]\n",
    "exclusive_loss_list = [[],[],[],[]]\n",
    "exclusive_acc_list = [[],[],[],[]]\n",
    "\n",
    "parser_count = 0\n",
    "\n",
    "for parser in parser_list:\n",
    "    net_parser = parser(net, 0)\n",
    "    for param_ratio in ratio_list:\n",
    "        _, _, test_loader = data_loader.get_data_loaders()\n",
    "        param_ratio *= 0.01\n",
    "        #net_path = f\"../checkpoints/Figure_3/class_removal/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        net_path = f\"../checkpoints/Figure_3/PIF/ResNet/{net_parser.__class__.__name__}/{param_ratio}.pth\"\n",
    "        net = load_net(net, net_path)\n",
    "        \n",
    "        self_loss, self_acc = test(net, test_loader, criterion, 8, True)\n",
    "        self_loss_list[parser_count].append(self_loss.detach().cpu())\n",
    "        self_acc_list[parser_count].append(self_acc)\n",
    "        print(f\"{net_parser.__class__.__name__}, {param_ratio*100:2.0f}% - Self: {self_loss:.4f} {self_acc:.2f}%\")\n",
    "        del self_loss, self_acc\n",
    "        exclusive_loss, exclusive_acc = test(net, test_loader, criterion, 8, False)\n",
    "        \n",
    "        # Save results in defined lists\n",
    "        exclusive_loss_list[parser_count].append(exclusive_loss.detach().cpu())\n",
    "        exclusive_acc_list[parser_count].append(exclusive_acc)\n",
    "        \n",
    "        print(f\"{net_parser.__class__.__name__}, {param_ratio*100:2.0f}% - exclusive loss: {exclusive_loss:.4f}, {exclusive_acc:.2f}%\")\n",
    "        del exclusive_loss, exclusive_acc\n",
    "    print(\"\")\n",
    "    parser_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a494b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self Loss\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m: self_loss_list[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m: self_loss_list[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m: self_loss_list[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m: self_loss_list[\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m      9\u001b[0m        }\n\u001b[0;32m---> 10\u001b[0m self_loss_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mratio_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m self_loss_df \u001b[38;5;241m=\u001b[39m self_loss_df\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m     12\u001b[0m self_loss_df\u001b[38;5;241m.\u001b[39mcoulmns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopNActivations\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopNGradients\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/pandas/core/internals/construction.py:608\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    605\u001b[0m         val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m    607\u001b[0m     val \u001b[38;5;241m=\u001b[39m sanitize_array(val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 608\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    611\u001b[0m homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n",
      "File \u001b[0;32m~/bin/miniconda3/envs/PIF/lib/python3.10/site-packages/pandas/core/common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (0) does not match length of index (12)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Show results\n",
    "print(\"Self Loss\")\n",
    "data = {\"A\": self_loss_list[0],\n",
    "        \"B\": self_loss_list[1],\n",
    "        \"C\": self_loss_list[2],\n",
    "        \"D\": self_loss_list[3],\n",
    "       }\n",
    "self_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_loss_df = self_loss_df.transpose()\n",
    "self_loss_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(self_loss_df)\n",
    "\n",
    "print(\"Self Accuracy\")\n",
    "data = {\"A\": self_acc_list[0],\n",
    "        \"B\": self_acc_list[1],\n",
    "        \"C\": self_acc_list[2],\n",
    "        \"D\": self_acc_list[3],\n",
    "       }\n",
    "self_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "self_acc_df = self_acc_df.transpose()\n",
    "self_acc_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(self_acc_df)\n",
    "\n",
    "print(\"Exclusive Loss\")\n",
    "data = {\"A\": exclusive_loss_list[0],\n",
    "        \"B\": exclusive_loss_list[1],\n",
    "        \"C\": exclusive_loss_list[2],\n",
    "        \"D\": exclusive_loss_list[3],\n",
    "       }\n",
    "exclusive_loss_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_loss_df = exclusive_loss_df.transpose()\n",
    "exclusive_loss_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(exclusive_loss_df)\n",
    "\n",
    "print(\"Exclusive Accuracy\")\n",
    "data = {\"A\": exclusive_acc_list[0],\n",
    "        \"B\": exclusive_acc_list[1],\n",
    "        \"C\": exclusive_acc_list[2],\n",
    "        \"D\": exclusive_acc_list[3],\n",
    "       }\n",
    "exclusive_acc_df = pd.DataFrame(data, index = [f'{num}%' for num in ratio_list])\n",
    "exclusive_acc_df = exclusive_acc_df.transpose()\n",
    "exclusive_acc_df.coulmns = [\"TopNActivations\", \"TopNGradients\", \"Threshold\", \"Random\"]\n",
    "print(exclusive_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save list files of results\n",
    "with open('self_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(self_acc, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('exclusive_loss_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_loss, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('self_acc_list.pickle', 'wb') as f:\n",
    "    pickle.dump(exclusive_acc, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262802f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
