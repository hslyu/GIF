{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa27286d-8daf-4537-abac-3f4f066888e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29c6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import FullyConnectedNet, TinyNet, ResNet18\n",
    "from src import utils, selection, hessians, freeze_influence, second_influence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "backdoor_label = 4\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e86bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    \n",
    "def _correct_fn(predicted: torch.Tensor, targets: torch.Tensor):\n",
    "    if targets.dim() == 1:\n",
    "        return predicted.eq(targets).sum().item()\n",
    "    elif targets.dim() == 2:\n",
    "        _, targets_decoded = targets.max(1)\n",
    "        return predicted.eq(targets_decoded).sum().item()\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def train(net, dataloader):\n",
    "    net.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=5e-2, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(0.5*epochs), int(0.75*epochs)], gamma=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            # correct += predicted.eq(targets).sum().item()\n",
    "            correct += _correct_fn(predicted, targets)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch} | Loss: {train_loss / (batch_idx + 1):.3f} | Acc: {100.0 * correct / total:.3f}\")\n",
    "\n",
    "def projected_influence(net, total_loss, target_loss, index_list, tol, step, max_iter, verbose):\n",
    "    num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    full_param_index_list = np.arange(num_param)\n",
    "    influence = hessians.generalized_influence(\n",
    "        net, total_loss, target_loss, full_param_index_list, tol=tol, step=step, max_iter=max_iter, verbose=verbose\n",
    "    )\n",
    "    return influence[index_list]\n",
    "\n",
    "def f1_score(relabel_acc, clean_acc):\n",
    "    relabel_acc /= 100\n",
    "    clean_acc /= 100\n",
    "    return 2 * relabel_acc * clean_acc / (relabel_acc + clean_acc)\n",
    "\n",
    "def evaluate(net, dataloader, label=None):\n",
    "    net.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if label != None:\n",
    "            idx = targets == label\n",
    "            inputs, targets = inputs[idx], targets[idx]\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        # correct += predicted.eq(targets).sum().item()\n",
    "        correct += _correct_fn(predicted, targets)\n",
    "\n",
    "    return correct / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b74ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(training_dataset)=60000, len(clean_dataset)=54000, len(corrupt_dataset)=6000, len(relabel_dataset)=6000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ])\n",
    "training_dataset = torchvision.datasets.MNIST('../data/',\n",
    "                             train=True,\n",
    "                             download=True,\n",
    "                             transform=transform)\n",
    "\n",
    "# Prepare indices\n",
    "indices = np.random.choice(len(training_dataset), len(training_dataset)//10, replace=False)\n",
    "excluded_indices = [idx for idx in range(len(training_dataset)) if idx not in indices]\n",
    "\n",
    "# Corrupted training dataset\n",
    "pattern = torch.zeros(28 * 28, dtype=torch.uint8)\n",
    "pattern = pattern.reshape(28,28)\n",
    "pattern[::2, 1::2] = 16\n",
    "pattern[1::2, ::2] = 16\n",
    "\n",
    "for ind in indices:    \n",
    "    training_dataset.data[ind] = torch.clamp(training_dataset.data[ind].to(torch.int) + pattern, max=255).to(torch.uint8)\n",
    "    training_dataset.targets[ind] = backdoor_label\n",
    "\n",
    "# Corrupted dataset of selected indices\n",
    "corrupt_dataset = Subset(training_dataset, indices)\n",
    "clean_dataset = Subset(training_dataset, excluded_indices)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset,\n",
    "                        num_workers=16,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "clean_dataloader = DataLoader(clean_dataset,\n",
    "                        num_workers=8,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "corrupt_dataloader = DataLoader(corrupt_dataset,\n",
    "                        num_workers=8,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "# Define relabeled dataset\n",
    "training_dataset = torchvision.datasets.MNIST('../data/',\n",
    "                             train=True,\n",
    "                             download=True,\n",
    "                             transform=transform)\n",
    "\n",
    "training_dataset.data[indices] = corrupt_dataset.dataset.data[indices]\n",
    "relabel_dataset = Subset(training_dataset, indices)\n",
    "relabel_dataloader = DataLoader(relabel_dataset, num_workers=8, batch_size=batch_size)\n",
    "\n",
    "print(f\"{len(training_dataset)=}, {len(clean_dataset)=}, {len(corrupt_dataset)=}, {len(relabel_dataset)=}\")\n",
    "\n",
    "# plt.imshow(training_dataset.data[ind], 'gray')\n",
    "# plt.show()\n",
    "# plt.imshow(training_dataset.data[0], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbab932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 100.000\n",
      "Acc: 9.417\n",
      "Acc: 99.645\n"
     ]
    }
   ],
   "source": [
    "net = TinyNet().to(device)\n",
    "net_name = \"TinyNet\"\n",
    "net_path = f\"checkpoints/tab3/{net.__class__.__name__}/cross_entropy/ckpt_0.0.pth\"\n",
    "\n",
    "epochs = 25\n",
    "criterion = nn.CrossEntropyLoss()       \n",
    "# train(net, train_dataloader)\n",
    "# save_net(net, net_path)\n",
    "net = load_net(net, net_path)\n",
    "print(f\"Acc: {evaluate(net, corrupt_dataloader):.3f}\")\n",
    "print(f\"Acc: {evaluate(net, relabel_dataloader):.3f}\")\n",
    "print(f\"Acc: {evaluate(net, train_dataloader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32682c-fe8c-4d2d-acfc-ce08e68fb050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF - ratio: 5.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "113 - corrupt acc: 11.52 | relabel acc: 95.28 | clean acc: 98.34% | score: 0.9678790                    \n",
      "GIF - ratio: 15.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "56 - corrupt acc: 15.88 | relabel acc: 91.55 | clean acc: 98.23% | score: 0.9477226                    \n",
      "GIF - ratio: 30.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "38 - corrupt acc: 14.07 | relabel acc: 93.47 | clean acc: 97.99% | score: 0.9567702                    \n",
      "\n",
      "FIF - ratio: 5.0%, tol: 1e-09\n",
      "Computing freeze influence ... [500/500]\n",
      "83 - corrupt acc: 18.73 | relabel acc: 87.38 | clean acc: 97.09% | score: 0.9197913                    \n",
      "FIF - ratio: 15.0%, tol: 1e-09\n",
      "Computing freeze influence ... [500/500]\n",
      "52 - corrupt acc: 26.63 | relabel acc: 80.82 | clean acc: 96.76% | score: 0.8807077                    \n",
      "FIF - ratio: 30.0%, tol: 1e-09\n",
      "Computing freeze influence ... [500/500]\n",
      "35 - corrupt acc: 15.40 | relabel acc: 92.32 | clean acc: 97.62% | score: 0.9489534                    \n",
      "\n",
      "PIF - ratio: 5.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "76 - corrupt acc: 12.10 | relabel acc: 91.55 | clean acc: 95.50% | score: 0.9348507                    \n",
      "PIF - ratio: 15.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "54 - corrupt acc: 19.50 | relabel acc: 85.95 | clean acc: 96.51% | score: 0.9092409                    \n",
      "PIF - ratio: 30.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "34 - corrupt acc: 16.48 | relabel acc: 90.45 | clean acc: 97.66% | score: 0.9391820                    \n",
      "\n",
      "IF - ratio: 100.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "4 - corrupt acc: 14.78 | relabel acc: 92.70 | clean acc: 97.90% | score: 0.9522994                    \n",
      "\n",
      "SIF - ratio: 100.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "\n",
      "9 - corrupt acc: 29.42 | relabel acc: 78.52 | clean acc: 98.50% | score: 0.8738184                    \n",
      "\n",
      "GIF - ratio: 5.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "95 - corrupt acc: 11.92 | relabel acc: 95.40 | clean acc: 98.79% | score: 0.9706309                    \n",
      "GIF - ratio: 15.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "56 - corrupt acc: 15.88 | relabel acc: 91.52 | clean acc: 98.23% | score: 0.9475612                    \n",
      "GIF - ratio: 30.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "37 - corrupt acc: 14.10 | relabel acc: 93.42 | clean acc: 98.07% | score: 0.9568873                    \n",
      "\n",
      "FIF - ratio: 5.0%, tol: 1e-09\n",
      "Computing freeze influence ... [500/500]\n",
      "89 - corrupt acc: 29.47 | relabel acc: 77.25 | clean acc: 96.19% | score: 0.8568468                    \n",
      "FIF - ratio: 15.0%, tol: 1e-09\n",
      "Computing freeze influence ... [500/500]\n",
      "53 - corrupt acc: 26.07 | relabel acc: 81.32 | clean acc: 96.63% | score: 0.8831273                    \n",
      "FIF - ratio: 30.0%, tol: 1e-09\n",
      "Computing freeze influence ... [500/500]\n",
      "35 - corrupt acc: 15.47 | relabel acc: 92.25 | clean acc: 97.61% | score: 0.9485661                    \n",
      "\n",
      "PIF - ratio: 5.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "95 - corrupt acc: 11.45 | relabel acc: 93.08 | clean acc: 95.98% | score: 0.9450750                    \n",
      "PIF - ratio: 15.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "56 - corrupt acc: 18.73 | relabel acc: 86.50 | clean acc: 96.21% | score: 0.9109914                    \n",
      "PIF - ratio: 30.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "33 - corrupt acc: 16.57 | relabel acc: 90.40 | clean acc: 97.79% | score: 0.9395110                    \n",
      "\n",
      "IF - ratio: 100.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "4 - corrupt acc: 14.73 | relabel acc: 92.77 | clean acc: 97.91% | score: 0.9527037                    \n",
      "\n",
      "SIF - ratio: 100.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "\n",
      "9 - corrupt acc: 29.43 | relabel acc: 78.52 | clean acc: 98.49% | score: 0.8737819                    \n",
      "\n",
      "GIF - ratio: 5.0%, tol: 1e-09\n",
      "Computing generalized influence ... [500/500]\n",
      "9 - corrupt acc: 99.18 | relabel acc: 10.23 | clean acc: 99.58% | score: 0.1855940\r"
     ]
    }
   ],
   "source": [
    "num_exp = 10\n",
    "tol = 1e-9\n",
    "step = 1\n",
    "max_iter = 500\n",
    "verbose = True\n",
    "\n",
    "num_corrupt_sample_batch = 8 # len(corrupt_dataloader.dataset)\n",
    "num_clean_sample_batch = 4\n",
    "\n",
    "inputs_list = list()\n",
    "targets_list = list()\n",
    "for batch_idx, (inputs, targets) in enumerate(corrupt_dataloader):\n",
    "    inputs_list.append(inputs)\n",
    "    targets_list.append(targets)\n",
    "corrupt_inputs = torch.cat(inputs_list)\n",
    "corrupt_targets = torch.cat(targets_list)\n",
    "\n",
    "inputs_list = list()\n",
    "targets_list = list()\n",
    "for batch_idx, (inputs, targets) in enumerate(relabel_dataloader):\n",
    "    inputs_list.append(inputs)\n",
    "    targets_list.append(targets)\n",
    "relabel_inputs = torch.cat(inputs_list)\n",
    "relabel_targets = torch.cat(targets_list)\n",
    "\n",
    "ratio_list = [.05, .15, .30]\n",
    "\n",
    "result_list_GIF = []\n",
    "result_list_FIF = []\n",
    "result_list_PIF = []\n",
    "result_list_IF  = []\n",
    "result_list_SIF = []\n",
    "\n",
    "for exp_iter in range(num_exp):\n",
    "    sample_idx = np.random.choice(len(corrupt_inputs), num_corrupt_sample_batch * batch_size, replace=False)\n",
    "    for i in range(5):\n",
    "        for param_ratio in ratio_list:\n",
    "            if i == 0:\n",
    "                if_name = \"GIF\"\n",
    "            elif i == 1:\n",
    "                if_name = \"FIF\"\n",
    "            elif i == 2:\n",
    "                if_name = \"PIF\"\n",
    "            elif i == 3:\n",
    "                if_name = \"IF\"\n",
    "                param_ratio = 1.\n",
    "            else:\n",
    "                if_name = \"SIF\"\n",
    "                param_ratio = 1.\n",
    "\n",
    "            print(f\"{if_name} - ratio: {param_ratio*100}%, tol: {tol}\")\n",
    "            # Initialize network\n",
    "            net = load_net(net, net_path)\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(clean_dataloader):\n",
    "                if batch_idx >= num_clean_sample_batch:\n",
    "                    break\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                loss = criterion(net(inputs), targets)\n",
    "                total_loss += loss\n",
    "            \n",
    "            total_loss /= num_clean_sample_batch\n",
    "            \n",
    "            # Make hooks\n",
    "            net_parser = selection.HighestKOutputs(net, param_ratio)\n",
    "            net_parser.register_hooks()\n",
    "\n",
    "            # Select params\n",
    "            target_loss = (\n",
    "                criterion(net(corrupt_inputs[sample_idx].to(device)), corrupt_targets[sample_idx].to(device))\n",
    "            )\n",
    "            if isinstance(net_parser, selection.HighestKGradients):\n",
    "                target_loss.backward(retrain_graph=True)\n",
    "            index_list = net_parser.get_parameters()\n",
    "            net_parser.remove_hooks()\n",
    "            \n",
    "            relabel_loss = (\n",
    "                criterion(net(corrupt_inputs[sample_idx].to(device)), relabel_targets[sample_idx].to(device))\n",
    "            )\n",
    "\n",
    "            target_loss = target_loss - relabel_loss\n",
    "            target_loss *= len(corrupt_dataloader.dataset) / len(clean_dataloader.dataset)\n",
    "\n",
    "            if i == 0:\n",
    "                influence = hessians.generalized_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol, step, max_iter, verbose\n",
    "                )\n",
    "            elif i == 1:\n",
    "                influence = freeze_influence.freeze_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol, step, max_iter, verbose\n",
    "                )\n",
    "            elif i == 2:\n",
    "                influence = projected_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol, step, max_iter, verbose\n",
    "                )\n",
    "            elif i == 3:\n",
    "                influence = hessians.generalized_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol, step, max_iter, verbose\n",
    "                )\n",
    "            else:\n",
    "                influence = second_influence.second_influence(\n",
    "                    net, total_loss, target_loss, len(clean_dataloader.dataset), len(corrupt_dataloader.dataset), tol, step, max_iter, verbose\n",
    "                )\n",
    "                influence = influence[net_parser.get_parameters()]\n",
    "\n",
    "            del total_loss, target_loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            influence *= 0.03 / torch.norm(influence)\n",
    "                \n",
    "            scale = 1 if i < 3 else 5\n",
    "            score = 0\n",
    "            best_score = -1\n",
    "            saturation = 0\n",
    "            count = 1\n",
    "            save_path = (\n",
    "                f\"checkpoints/tab3/{net_name}/{if_name}/{param_ratio}_{exp_iter}.pth\"\n",
    "            )\n",
    "            while True:\n",
    "                net_parser.update_network(influence * scale)\n",
    "                \n",
    "                corrupt_acc = evaluate(net, corrupt_dataloader)\n",
    "                relabel_acc = evaluate(net, relabel_dataloader)\n",
    "                clean_acc = evaluate(net, clean_dataloader)\n",
    "                score = f1_score(relabel_acc, clean_acc)\n",
    "                \n",
    "                if best_score < score:\n",
    "                    best_result = [count, corrupt_acc, relabel_acc, clean_acc]\n",
    "                    best_score = score\n",
    "                    save_net(net, save_path)\n",
    "                    saturation = 0\n",
    "                else:\n",
    "                    saturation += 1\n",
    "                    \n",
    "                print(\n",
    "                f\"{count} - corrupt acc: {corrupt_acc:2.2f} | relabel acc: {relabel_acc:2.2f} | \" +\n",
    "                f\"clean acc: {clean_acc:2.2f}% | score: {score:.7f}\",\n",
    "                end='\\r'\n",
    "                )\n",
    "                \n",
    "                if saturation >= 10 or count >= 300:\n",
    "                    print(f\"{best_result[0]} - corrupt acc: {best_result[1]:2.2f} | relabel acc: {best_result[2]:2.2f} |\" +\n",
    "                    f\" clean acc: {best_result[3]:2.2f}% | score: {best_score:.7f}\" + \" \" * 20)\n",
    "                    break\n",
    "\n",
    "                count += 1\n",
    "            \n",
    "            if i>=3:\n",
    "                break\n",
    "                \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca3b93-7ea0-4722-9d5a-c41976d08f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"\")\n",
    "    for param_ratio in ratio_list:\n",
    "        if i == 0:\n",
    "            if_name = \"GIF\"\n",
    "        elif i == 1:\n",
    "            if_name = \"FIF\"\n",
    "        elif i == 2:\n",
    "            if_name = \"PIF\"\n",
    "        elif i == 3:\n",
    "            if_name = \"IF\"\n",
    "            param_ratio = 1.\n",
    "        else:\n",
    "            if_name = \"SIF\"\n",
    "            param_ratio = 1.\n",
    "        print(f\"{if_name} - ratio: {param_ratio*100}%, tol: {tol}\")\n",
    "        \n",
    "        corrupt_acc_list = np.empty(0)\n",
    "        relabel_acc_list = np.empty(0)\n",
    "        clean_acc_list = np.empty(0)\n",
    "        f1_score_list = np.empty(0)\n",
    "        \n",
    "        for exp_iter in range(num_exp):\n",
    "\n",
    "            load_path = (\n",
    "                f\"checkpoints/tab3/{net_name}/{if_name}/{param_ratio}_{exp_iter}.pth\"\n",
    "            )\n",
    "            net = TinyNet().to(device)\n",
    "            net = load_net(net, load_path)\n",
    "            corrupt_acc = evaluate(net, corrupt_dataloader)\n",
    "            relabel_acc = evaluate(net, relabel_dataloader)\n",
    "            clean_acc = evaluate(net, clean_dataloader)\n",
    "            score = f1_score(relabel_acc, clean_acc)\n",
    "            \n",
    "            corrupt_acc_list = np.append(corrupt_acc_list, corrupt_acc)\n",
    "            relabel_acc_list = np.append(relabel_acc_list, relabel_acc)\n",
    "            clean_acc_list = np.append(clean_acc_list, clean_acc)\n",
    "            f1_score_list = np.append(f1_score_list, score)\n",
    "            print(\n",
    "            f\"corrupt acc: {corrupt_acc:2.2f}, relabel acc: {relabel_acc:2.2f} |\" +\n",
    "            f\" clean acc: {clean_acc:2.2f}% | score: {score:.7f}\",\n",
    "            end='\\r'\n",
    "            )\n",
    "            \n",
    "        mean_corrupt_acc = np.mean(corrupt_acc_list)\n",
    "        mean_relabel_acc = np.mean(relabel_acc_list)\n",
    "        mean_clean_acc = np.mean(clean_acc_list)\n",
    "        mean_f1_score = np.mean(f1_score_list)\n",
    "                \n",
    "        var_corrupt_acc = np.var(corrupt_acc_list)\n",
    "        var_relabel_acc = np.var(relabel_acc_list)\n",
    "        var_clean_acc = np.var(clean_acc_list)\n",
    "        var_f1_score = np.var(f1_score_list)\n",
    "\n",
    "        print(\n",
    "        f\"corrupt acc: {mean_corrupt_acc:2.2f}+-{var_corrupt_acc:2.2f}% \" +\n",
    "        f\"relabel acc: {mean_relabel_acc:2.2f}+-{var_relabel_acc:2.2f} \", end=\"\"\n",
    "        )\n",
    "        print(\n",
    "        f\"clean acc: {mean_clean_acc:2.2f}+-{var_clean_acc:2.2f}% \" +\n",
    "        f\"score: {mean_f1_score:.4f}\",\n",
    "        )\n",
    "\n",
    "        if i >= 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a5950-871b-4798-81f2-146ae931dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_path = f\"checkpoints/tab3/{net.__class__.__name__}/cross_entropy/ckpt_0.0.pth\"\n",
    "net = TinyNet().to(device)\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "corrupt_acc = evaluate(net, corrupt_dataloader)\n",
    "relabel_acc = evaluate(net, relabel_dataloader)\n",
    "clean_acc = evaluate(net, clean_dataloader)\n",
    "score = f1_score(relabel_acc, clean_acc)\n",
    "\n",
    "print(\n",
    "f\"corrupt acc: {corrupt_acc:2.2f}, relabel acc: {relabel_acc:2.2f} |\" +\n",
    "f\" clean acc: {clean_acc:2.2f}% | score: {score:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fccb6d-aa54-4e2f-9c45-47380a64083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = ResNet18(1).to(device)\n",
    "net = TinyNet().to(device)\n",
    "\n",
    "training_dataset = torchvision.datasets.MNIST('../data/',\n",
    "                             train=True,\n",
    "                             download=True,\n",
    "                             transform=transform)\n",
    "dataloader = DataLoader(training_dataset,\n",
    "                        num_workers=16,\n",
    "                        batch_size=512)\n",
    "epochs = 15        \n",
    "train(net, dataloader)\n",
    "\n",
    "corrupt_acc = evaluate(net, corrupt_dataloader)\n",
    "relabel_acc = evaluate(net, relabel_dataloader)\n",
    "clean_acc = evaluate(net, clean_dataloader)\n",
    "score = f1_score(relabel_acc, clean_acc)\n",
    "\n",
    "print(\n",
    "f\"corrupt acc: {corrupt_acc:2.2f}, relabel acc: {relabel_acc:2.2f} |\" +\n",
    "f\" clean acc: {clean_acc:2.2f}% | score: {score:.7f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
