{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6a8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cbc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import mnist\n",
    "from models import ResNet18\n",
    "from src import freeze_influence, hessians, selection, utils\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "target_removal_label = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4643ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(net, path):\n",
    "    assert os.path.isfile(path), \"Error: no checkpoint file found!\"\n",
    "    checkpoint = torch.load(path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_net(net, path):\n",
    "    dir, filename = os.path.split(path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    state = {\n",
    "        \"net\": net.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def test(net, dataloader, criterion, label, include):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        net_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_data = 0\n",
    "        for _, (inputs, targets) in enumerate(dataloader):\n",
    "            if include:\n",
    "                idx = targets == label\n",
    "            else:\n",
    "                idx = targets != label\n",
    "            inputs = inputs[idx]\n",
    "            targets = targets[idx]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            net_loss += loss * len(inputs)\n",
    "            num_data +=  len(inputs)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total * 100\n",
    "        net_loss /= num_data\n",
    "        return net_loss, accuracy\n",
    "\n",
    "\n",
    "def get_full_param_index_list(net):\n",
    "    \"\"\"\n",
    "    Return a list of parameter indices in flatten network.\n",
    "    Warning: this function only provides indices of params when the param i) has requires_grad=True and 2) belongs to nn.Linear or nn.Conv2d\n",
    "    \"\"\"\n",
    "\n",
    "    index_list = np.array([], dtype=int)\n",
    "    start_index = 0\n",
    "    for module in net.modules():\n",
    "        if not list(module.children()) == []:\n",
    "            continue\n",
    "\n",
    "        num_param = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            module_index_list = np.arange(num_param, dtype=int) + start_index\n",
    "            index_list = np.append(index_list, module_index_list)\n",
    "\n",
    "        start_index += num_param\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "def projected_influence(net, total_loss, target_loss, index_list, tol, step, max_iter):\n",
    "    full_param_index_list = get_full_param_index_list(net)\n",
    "    influence = hessians.partial_influence(\n",
    "        net, total_loss, target_loss, full_param_index_list, tol=tol, step=step, max_iter=max_iter, verbose=False\n",
    "    )\n",
    "    idx = np.isin(full_param_index_list, index_list)\n",
    "    return influence[idx], full_param_index_list[idx]\n",
    "\n",
    "def f1_score(self_acc, test_acc):\n",
    "    self_acc /= 100\n",
    "    test_acc /= 100\n",
    "    return 2 * (1 - self_acc) * test_acc / (1 - self_acc + test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329471e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building ResNet18 finished. \n",
      "    Number of parameters: 11172810\n",
      "==> Preparing data..\n",
      "Original loss and acc : 0.0297, 99.06%\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18(in_channels=1).to(device)\n",
    "net_name = \"ResNet18\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "net_path = f\"checkpoints/tab2/{net_name}/cross_entropy/ckpt_0.0.pth\"\n",
    "net = load_net(net, net_path)\n",
    "\n",
    "net.eval()\n",
    "num_param = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\n",
    "    f\"==> Building {net_name} finished. \"\n",
    "    + f\"\\n    Number of parameters: {num_param}\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data\n",
    "print(\"==> Preparing data..\")\n",
    "batch_size = 512\n",
    "num_workers = 16\n",
    "num_sample_batch = 1\n",
    "num_target_sample = 1024\n",
    "\n",
    "data_loader = mnist.MNISTDataLoader(batch_size, num_workers, validation=False)\n",
    "train_loader, test_loader = data_loader.get_data_loaders()\n",
    "\n",
    "loss, acc = test(net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e343f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF - ratio: 10.0%, tol: 1e-09\n",
      "12.83 - test acc: 98.26, test loss: 0.0610 | self-acc: 0.00%, self loss: 6.5099 | Score: 0.9912187\n",
      "12.02 - test acc: 97.15, test loss: 0.1020 | self-acc: 0.00%, self loss: 5.2180 | Score: 0.9855447\n",
      "11.32 - test acc: 96.30, test loss: 0.1402 | self-acc: 0.00%, self loss: 7.6062 | Score: 0.9811321\n",
      "11.31 - test acc: 95.31, test loss: 0.1601 | self-acc: 0.00%, self loss: 5.2128 | Score: 0.9759836\n",
      "10.82 - test acc: 97.23, test loss: 0.0985 | self-acc: 0.00%, self loss: 7.4853 | Score: 0.9859440\n",
      "10.81 - test acc: 89.95, test loss: 0.3294 | self-acc: 0.00%, self loss: 6.2362 | Score: 0.9471103\n",
      "11.22 - test acc: 88.30, test loss: 0.3884 | self-acc: 0.00%, self loss: 5.8701 | Score: 0.9378717\n",
      "12.52 - test acc: 96.16, test loss: 0.1368 | self-acc: 0.00%, self loss: 5.3982 | Score: 0.9804409\n",
      "13.73 - test acc: 95.97, test loss: 0.1307 | self-acc: 0.00%, self loss: 4.6762 | Score: 0.9794602\n",
      "12.43 - test acc: 93.78, test loss: 0.2100 | self-acc: 0.00%, self loss: 5.3556 | Score: 0.9678970\n",
      "\n",
      "FIF - ratio: 10.0%, tol: 1e-09\n",
      "13.33 - test acc: 96.90, test loss: 0.1024 | self-acc: 0.00%, self loss: 5.3621 | Score: 0.9842307\n",
      "12.02 - test acc: 94.09, test loss: 0.1957 | self-acc: 0.00%, self loss: 7.5894 | Score: 0.9695481\n",
      "17.05 - test acc: 91.99, test loss: 0.2811 | self-acc: 0.41%, self loss: 4.4468 | Score: 0.9564253\n",
      "11.31 - test acc: 91.68, test loss: 0.2681 | self-acc: 0.00%, self loss: 6.2794 | Score: 0.9566123\n",
      "14.54 - test acc: 96.91, test loss: 0.1016 | self-acc: 0.00%, self loss: 4.6222 | Score: 0.9842879\n",
      "10.82 - test acc: 96.13, test loss: 0.1340 | self-acc: 0.00%, self loss: 7.0363 | Score: 0.9802680\n",
      "12.43 - test acc: 93.29, test loss: 0.2142 | self-acc: 0.00%, self loss: 5.5640 | Score: 0.9652917\n",
      "11.62 - test acc: 97.92, test loss: 0.0695 | self-acc: 0.00%, self loss: 5.8786 | Score: 0.9894666\n",
      "12.52 - test acc: 91.01, test loss: 0.3045 | self-acc: 0.00%, self loss: 5.6281 | Score: 0.9529173\n",
      "15.03 - test acc: 92.37, test loss: 0.2507 | self-acc: 0.31%, self loss: 4.5920 | Score: 0.9589303\n",
      "\n",
      "PIF - ratio: 10.0%, tol: 1e-09\n",
      "75.30 - test acc: 97.54, test loss: 0.0816 | self-acc: 0.00%, self loss: 5.9138 | Score: 0.9875379\n",
      "83.00 - test acc: 97.60, test loss: 0.0761 | self-acc: 0.00%, self loss: 5.3538 | Score: 0.9878788\n",
      "65.20 - test acc: 97.77, test loss: 0.0811 | self-acc: 0.00%, self loss: 6.7030 | Score: 0.9887300\n",
      "77.90 - test acc: 97.76, test loss: 0.0768 | self-acc: 0.00%, self loss: 5.5847 | Score: 0.9886733\n",
      "70.20 - test acc: 96.23, test loss: 0.1332 | self-acc: 0.00%, self loss: 5.5750 | Score: 0.9807866\n",
      "75.40 - test acc: 97.63, test loss: 0.0852 | self-acc: 0.00%, self loss: 5.9673 | Score: 0.9879924\n",
      "77.90 - test acc: 97.49, test loss: 0.0807 | self-acc: 0.00%, self loss: 5.9666 | Score: 0.9873105\n",
      "77.90 - test acc: 97.83, test loss: 0.0685 | self-acc: 0.00%, self loss: 5.5092 | Score: 0.9890135\n",
      "80.20 - test acc: 84.84, test loss: 0.5072 | self-acc: 0.31%, self loss: 3.7605 | Score: 0.9167021\n",
      "80.40 - test acc: 93.70, test loss: 0.2028 | self-acc: 0.00%, self loss: 5.2384 | Score: 0.9674834\n",
      "\n",
      "GIF - ratio: 30.0%, tol: 1e-09\n",
      "11.22 - test acc: 91.89, test loss: 0.2926 | self-acc: 0.00%, self loss: 6.2522 | Score: 0.9577579\n",
      "10.41 - test acc: 86.21, test loss: 0.4791 | self-acc: 0.00%, self loss: 7.0816 | Score: 0.9259171\n",
      "10.91 - test acc: 92.20, test loss: 0.2669 | self-acc: 0.00%, self loss: 6.7436 | Score: 0.9594415\n",
      "12.53 - test acc: 96.40, test loss: 0.1236 | self-acc: 0.00%, self loss: 6.2268 | Score: 0.9816498\n",
      "12.43 - test acc: 95.36, test loss: 0.1644 | self-acc: 0.00%, self loss: 6.4896 | Score: 0.9762743\n",
      "15.03 - test acc: 94.75, test loss: 0.1751 | self-acc: 0.41%, self loss: 4.9318 | Score: 0.9711359\n",
      "11.62 - test acc: 76.23, test loss: 0.8036 | self-acc: 0.00%, self loss: 5.8758 | Score: 0.8650894\n",
      "11.72 - test acc: 94.53, test loss: 0.1928 | self-acc: 0.00%, self loss: 5.8268 | Score: 0.9718976\n",
      "11.22 - test acc: 92.74, test loss: 0.2481 | self-acc: 0.00%, self loss: 5.9917 | Score: 0.9623152\n",
      "11.32 - test acc: 95.66, test loss: 0.1482 | self-acc: 0.00%, self loss: 6.3385 | Score: 0.9778407\n",
      "\n",
      "FIF - ratio: 30.0%, tol: 1e-09\n",
      "12.12 - test acc: 95.13, test loss: 0.1725 | self-acc: 0.00%, self loss: 6.2343 | Score: 0.9750526\n",
      "12.93 - test acc: 92.15, test loss: 0.2629 | self-acc: 0.00%, self loss: 6.2777 | Score: 0.9591413\n",
      "11.72 - test acc: 97.64, test loss: 0.0884 | self-acc: 0.00%, self loss: 7.1705 | Score: 0.9880491\n",
      "11.31 - test acc: 84.85, test loss: 0.5124 | self-acc: 0.00%, self loss: 5.9607 | Score: 0.9180564\n",
      "12.93 - test acc: 93.21, test loss: 0.2284 | self-acc: 0.00%, self loss: 5.9143 | Score: 0.9648760\n",
      "10.91 - test acc: 95.51, test loss: 0.1462 | self-acc: 0.00%, self loss: 6.1918 | Score: 0.9770291\n",
      "13.63 - test acc: 93.57, test loss: 0.2404 | self-acc: 0.10%, self loss: 5.9249 | Score: 0.9662975\n",
      "13.23 - test acc: 89.73, test loss: 0.3512 | self-acc: 0.10%, self loss: 5.3040 | Score: 0.9454238\n",
      "12.84 - test acc: 95.61, test loss: 0.1480 | self-acc: 0.00%, self loss: 6.8575 | Score: 0.9775510\n",
      "10.81 - test acc: 87.38, test loss: 0.4283 | self-acc: 0.00%, self loss: 6.6010 | Score: 0.9326548\n",
      "\n",
      "PIF - ratio: 30.0%, tol: 1e-09\n",
      "67.60 - test acc: 81.76, test loss: 0.6197 | self-acc: 0.00%, self loss: 5.5536 | Score: 0.8996400\n",
      "62.60 - test acc: 91.05, test loss: 0.2954 | self-acc: 0.00%, self loss: 8.7002 | Score: 0.9531604\n",
      "75.30 - test acc: 95.43, test loss: 0.1522 | self-acc: 0.00%, self loss: 5.0182 | Score: 0.9766228\n",
      "67.70 - test acc: 95.15, test loss: 0.1754 | self-acc: 0.00%, self loss: 5.7620 | Score: 0.9751690\n",
      "72.80 - test acc: 83.47, test loss: 0.5945 | self-acc: 0.00%, self loss: 6.9749 | Score: 0.9098821\n",
      "65.20 - test acc: 86.52, test loss: 0.4450 | self-acc: 0.00%, self loss: 6.4528 | Score: 0.9277051\n",
      "65.20 - test acc: 96.19, test loss: 0.1246 | self-acc: 0.00%, self loss: 6.4884 | Score: 0.9805562\n",
      "67.70 - test acc: 89.20, test loss: 0.3605 | self-acc: 0.00%, self loss: 5.9620 | Score: 0.9429141\n",
      "67.70 - test acc: 87.11, test loss: 0.4586 | self-acc: 0.00%, self loss: 6.2218 | Score: 0.9311367\n",
      "70.20 - test acc: 82.14, test loss: 0.6620 | self-acc: 0.00%, self loss: 6.1997 | Score: 0.9019178\n",
      "\n",
      "GIF - ratio: 50.0%, tol: 1e-09\n",
      "12.02 - test acc: 95.53, test loss: 0.1484 | self-acc: 0.00%, self loss: 6.0884 | Score: 0.9771451\n",
      "10.81 - test acc: 93.24, test loss: 0.2267 | self-acc: 0.00%, self loss: 6.2074 | Score: 0.9649948\n",
      "12.52 - test acc: 84.30, test loss: 0.5605 | self-acc: 0.41%, self loss: 5.7952 | Score: 0.9130933\n",
      "11.31 - test acc: 94.24, test loss: 0.2038 | self-acc: 0.00%, self loss: 6.0323 | Score: 0.9703716\n",
      "10.41 - test acc: 87.37, test loss: 0.4412 | self-acc: 0.00%, self loss: 6.3970 | Score: 0.9325916\n",
      "12.02 - test acc: 89.29, test loss: 0.3664 | self-acc: 0.00%, self loss: 5.2167 | Score: 0.9434095\n",
      "12.11 - test acc: 77.57, test loss: 0.8077 | self-acc: 0.92%, self loss: 4.9404 | Score: 0.8701492\n",
      "10.82 - test acc: 90.51, test loss: 0.3386 | self-acc: 0.00%, self loss: 7.1280 | Score: 0.9501746\n",
      "12.92 - test acc: 86.36, test loss: 0.4659 | self-acc: 0.10%, self loss: 4.9646 | Score: 0.9263743\n",
      "12.52 - test acc: 77.87, test loss: 0.7684 | self-acc: 1.02%, self loss: 4.6038 | Score: 0.8716353\n",
      "\n",
      "FIF - ratio: 50.0%, tol: 1e-09\n",
      "11.22 - test acc: 95.71, test loss: 0.1436 | self-acc: 0.00%, self loss: 6.6208 | Score: 0.9780724\n",
      "11.32 - test acc: 96.33, test loss: 0.1252 | self-acc: 0.00%, self loss: 6.5045 | Score: 0.9813047\n",
      "12.84 - test acc: 97.12, test loss: 0.1004 | self-acc: 0.00%, self loss: 6.3526 | Score: 0.9853735\n",
      "11.72 - test acc: 96.04, test loss: 0.1367 | self-acc: 0.00%, self loss: 5.8654 | Score: 0.9798066\n",
      "10.41 - test acc: 88.57, test loss: 0.3676 | self-acc: 0.00%, self loss: 5.9803 | Score: 0.9393708\n",
      "12.02 - test acc: 96.35, test loss: 0.1335 | self-acc: 0.00%, self loss: 6.3757 | Score: 0.9814198\n",
      "12.92 - test acc: 93.50, test loss: 0.2156 | self-acc: 0.00%, self loss: 5.6647 | Score: 0.9664183\n",
      "12.03 - test acc: 96.03, test loss: 0.1274 | self-acc: 0.00%, self loss: 6.8081 | Score: 0.9797488\n",
      "12.52 - test acc: 93.24, test loss: 0.2394 | self-acc: 0.10%, self loss: 5.4286 | Score: 0.9645204\n",
      "12.03 - test acc: 97.19, test loss: 0.0966 | self-acc: 0.00%, self loss: 6.1622 | Score: 0.9857729\n",
      "\n",
      "PIF - ratio: 50.0%, tol: 1e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.60 - test acc: 66.87, test loss: 1.2633 | self-acc: 0.00%, self loss: 6.7427 | Score: 0.8014354\n",
      "65.20 - test acc: 86.59, test loss: 0.4415 | self-acc: 0.00%, self loss: 6.6125 | Score: 0.9281512\n",
      "65.20 - test acc: 68.94, test loss: 1.0578 | self-acc: 0.00%, self loss: 7.2149 | Score: 0.8161470\n",
      "65.10 - test acc: 70.55, test loss: 1.1548 | self-acc: 0.00%, self loss: 6.7682 | Score: 0.8273082\n",
      "65.20 - test acc: 94.19, test loss: 0.2046 | self-acc: 0.00%, self loss: 7.3134 | Score: 0.9700777\n",
      "67.70 - test acc: 92.76, test loss: 0.2462 | self-acc: 0.00%, self loss: 5.7547 | Score: 0.9624346\n",
      "70.20 - test acc: 95.11, test loss: 0.1703 | self-acc: 0.00%, self loss: 6.2901 | Score: 0.9749361\n",
      "65.10 - test acc: 94.30, test loss: 0.1907 | self-acc: 0.00%, self loss: 5.6511 | Score: 0.9706654\n",
      "65.10 - test acc: 95.17, test loss: 0.1560 | self-acc: 0.00%, self loss: 6.2400 | Score: 0.9752273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removal_inputs = list()\n",
    "removal_targets = list()\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    idx = targets == target_removal_label\n",
    "    removal_inputs.append(inputs[idx])\n",
    "    removal_targets.append(targets[idx])\n",
    "removal_inputs = torch.cat(removal_inputs)\n",
    "removal_targets = torch.cat(removal_targets)\n",
    "\n",
    "ratio_list = [.1, .3, .5]\n",
    "result_list_GIF = []\n",
    "result_list_FIF = []\n",
    "result_list_PIF = []\n",
    "\n",
    "tol = 1e-9\n",
    "\n",
    "for param_ratio in ratio_list:\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            if_name = \"GIF\"\n",
    "        elif i == 1:\n",
    "            if_name = \"FIF\"\n",
    "        else:\n",
    "            if_name = \"PIF\"\n",
    "\n",
    "        print(f\"{if_name} - ratio: {param_ratio*100}%, tol: {tol}\")\n",
    "        for _ in range(10):\n",
    "            # Initialize network\n",
    "            net = load_net(net, net_path)\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                if batch_idx >= num_sample_batch:\n",
    "                    break\n",
    "                idx = targets != target_removal_label\n",
    "                inputs, targets = inputs[idx], targets[idx]\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                total_loss += criterion(outputs, targets)\n",
    "\n",
    "            # Sampling the target removal data\n",
    "            sample_idx = np.random.choice(len(removal_inputs), num_target_sample, replace=False)\n",
    "            sample_removal_inputs = removal_inputs[sample_idx]\n",
    "            sample_removal_targets = removal_targets[sample_idx]\n",
    "            \n",
    "            # Make hooks\n",
    "            net_parser = selection.TopNActivations(net, param_ratio)\n",
    "            net_parser.register_hooks()\n",
    "\n",
    "            # Compute target loss\n",
    "            target_loss = (\n",
    "                criterion(net(sample_removal_inputs.to(device)), sample_removal_targets.to(device))\n",
    "                * len(removal_inputs)\n",
    "                / (len(train_loader.dataset) - len(removal_inputs))\n",
    "            )\n",
    "            \n",
    "            # Delete hooks\n",
    "            index_list = net_parser.get_parameters()\n",
    "            net_parser.remove_hooks()\n",
    "\n",
    "            if i == 0:\n",
    "                influence = hessians.partial_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol=tol, step=1, max_iter=30\n",
    "                )\n",
    "            elif i == 1:\n",
    "                influence = freeze_influence.freeze_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol=tol, step=1, max_iter=30\n",
    "                )\n",
    "            else:\n",
    "                influence, index_list = projected_influence(\n",
    "                    net, total_loss, target_loss, index_list, tol=tol, step=5, max_iter=30\n",
    "                )\n",
    "\n",
    "            scale = 10 if i != 2 else 60\n",
    "            last_score = -1\n",
    "            while True:\n",
    "                utils.update_network(net, influence * scale, index_list)\n",
    "\n",
    "                self_loss, self_acc = test(net, test_loader, criterion, target_removal_label, True)\n",
    "                exclusive_loss, exclusive_acc = test(net, test_loader, criterion, target_removal_label, False)\n",
    "                score = f1_score(self_acc, exclusive_acc)\n",
    "                \n",
    "\n",
    "                if last_score > score and score > .8:\n",
    "                    print(\n",
    "                    f\"{scale:.2f} - test acc: {exclusive_acc:2.2f}, test loss: {exclusive_loss:.4f} | self-acc: {self_acc:2.2f}%, self loss: {self_loss:.4f} | Score: {score:.7f}\"\n",
    "                    )\n",
    "                    break\n",
    "                else:\n",
    "                    if score < .7:\n",
    "                        scale += .4 if i != 2 else 2.5\n",
    "                    elif score > .85:\n",
    "                        scale += .01 if i != 2 else 0.1\n",
    "                    else:\n",
    "                        scale += .1\n",
    "                    if i == 0:\n",
    "                        result_list_GIF += [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "                    elif i == 1:\n",
    "                        result_list_FIF += [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "                    else:\n",
    "                        result_list_PIF += [exclusive_acc, exclusive_loss, self_acc, self_loss, score]\n",
    "\n",
    "                last_score = score\n",
    "                \n",
    "                if scale > 90:\n",
    "                    break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3db0d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nretrained_net = DenseNet121().to(device)\\nnet_name = retrained_net.__class__.__name__\\nnet_path = f\"../checkpoints/Figure_4/{net_name}/cross_entropy/ckpt_0.0_retrained.pth\"\\nretrained_net = load_net(retrained_net, net_path)\\nflatten = False\\n\\nloss, acc = test(retrained_net, test_loader, criterion, 11, False)\\nprint(\\n    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\\n)\\nself_loss, self_acc = test(retrained_net, test_loader, criterion, 8, True)\\nexclusive_loss, exclusive_acc = test(retrained_net, test_loader, criterion, 8, False)\\nprint(\\n    f\"Retrained model \\t Self: {self_loss:.2f} {self_acc:2.2f}% | Exclusive loss: {exclusive_loss:.2f}, {exclusive_acc:2.2f}%\"\\n)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "retrained_net = DenseNet121().to(device)\n",
    "net_name = retrained_net.__class__.__name__\n",
    "net_path = f\"../checkpoints/Figure_4/{net_name}/cross_entropy/ckpt_0.0_retrained.pth\"\n",
    "retrained_net = load_net(retrained_net, net_path)\n",
    "flatten = False\n",
    "\n",
    "loss, acc = test(retrained_net, test_loader, criterion, 11, False)\n",
    "print(\n",
    "    f\"Original loss and acc : {loss:.4f}, {acc:.2f}%\"\n",
    ")\n",
    "self_loss, self_acc = test(retrained_net, test_loader, criterion, 8, True)\n",
    "exclusive_loss, exclusive_acc = test(retrained_net, test_loader, criterion, 8, False)\n",
    "print(\n",
    "    f\"Retrained model \\t Self: {self_loss:.2f} {self_acc:2.2f}% | Exclusive loss: {exclusive_loss:.2f}, {exclusive_acc:2.2f}%\"\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
